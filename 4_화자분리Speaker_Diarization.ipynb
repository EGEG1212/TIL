{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.화자분리Speaker_Diarization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPeqp+lhCjayoRUmoKT5zGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EGEG1212/TIL_AudioSpeechProcessing/blob/main/4_%ED%99%94%EC%9E%90%EB%B6%84%EB%A6%ACSpeaker_Diarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6odWig_sCVJd"
      },
      "source": [
        "# 화자분리(Speaker Diarization)\n",
        "- Speaker Diarization(화자 분리)는 오디오에서 각 부분에서의 화자를 인식하는 기술 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApAbzolqC58N"
      },
      "source": [
        "## UIS-RNN\n",
        " - 대표적인 모델 Unbounded Interleaved-State Recurrent Neural Network(UIS-RNNMM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JejExUcMC6Xd"
      },
      "source": [
        "## 라이브러리 설치\n",
        "- uisrnn을 간편하게 구현할 수 있는 uisrnn 라이브러리가 존재\n",
        "- 여기서는 uisrnn 라이브러리를 통해 uisrnn을 구현하고 학습, 평가\n",
        "- 실습을 위해 uisrnn과 easydict 라이브러리를 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5DrlXrQC6gY",
        "outputId": "bc601eb6-ea6c-4feb-cb3d-c544a76480fd"
      },
      "source": [
        "!pip install uisrnn easydict"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting uisrnn\n",
            "  Downloading https://files.pythonhosted.org/packages/49/ac/240e688480bbcb541458642e36b2211ac58fa17e8c9239cc55e22a244822/uisrnn-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n",
            "Installing collected packages: uisrnn\n",
            "Successfully installed uisrnn-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfybQBR3C6k9"
      },
      "source": [
        "## 데이터 다운로드\n",
        "- 학습 및 평가에는 uisrnn 라이브러리에서 제공하는 sample dataset을 사용\n",
        "- urlretrieve를 통해  url에서 데이터를 받아옴\n",
        "- <https://github.com/google/uis-rnn/blob/master/data/toy_training_data.npz?raw=True>\n",
        "- <https://github.com/google/uis-rnn/blob/master/data/toy_testing_data.npz?raw=True>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1wxu58-C6oz",
        "outputId": "9ef46437-3750-481b-9634-1da31d62655b"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "training_url = 'https://github.com/google/uis-rnn/blob/master/data/toy_training_data.npz?raw=True'\n",
        "urllib.request.urlretrieve(training_url, './toy_training_data.npz')\n",
        "\n",
        "testing_url = 'https://github.com/google/uis-rnn/blob/master/data/toy_testing_data.npz?raw=True'\n",
        "urllib.request.urlretrieve(testing_url, './toy_testing_data.npz')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./toy_testing_data.npz', <http.client.HTTPMessage at 0x7f75a9d6e0d0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zie735wgC6sn"
      },
      "source": [
        "- 다운로드한 데이터에서 데이터를 받아오고 sequence와 label을 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Pf61IaNC6wM"
      },
      "source": [
        "import numpy as np\n",
        "import uisrnn\n",
        "\n",
        "train_data = np.load('./toy_training_data.npz', allow_pickle=True)\n",
        "test_data = np.load('./toy_testing_data.npz', allow_pickle=True)\n",
        "\n",
        "train_sequence = train_data['train_sequence']\n",
        "train_cluster_id = train_data['train_cluster_id']\n",
        "\n",
        "test_sequences = test_data['test_sequences'].tolist()\n",
        "test_cluster_ids = test_data['test_cluster_ids'].tolist()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S77eKkb_C6z9"
      },
      "source": [
        "## 파라미터 설정\n",
        "- model, training, inference에 필요한 인자들은 uisrnn.parse_arguments()를 통해 얻을 수 있는데...\n",
        "- colab환경에서는 argument를 사용할 수 없기 때문에 easydict로 대체"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twAUEyAXC63s"
      },
      "source": [
        "import easydict\n",
        "\n",
        "model_args = easydict.EasyDict({\"crp_alpha\": 1.0,\n",
        "                                \"enable_cuda\": True,\n",
        "                                \"observation_dim\": 256,\n",
        "                                \"rnn_depth\": 1,\n",
        "                                \"rnn_dropout\": 0.2,\n",
        "                                \"rnn_hidden_size\": 512,\n",
        "                                \"sigma2\": None,\n",
        "                                \"transition_bias\": None,\n",
        "                                \"verbosity\": 2})\n",
        "\n",
        "training_args = easydict.EasyDict({\"batch_size\": 10,\n",
        "                                   \"enforce_cluster_id_uniqueness\": True,\n",
        "                                   \"grad_max_norm\": 5.0,\n",
        "                                   \"learning_rate\": 0.001,\n",
        "                                   \"num_permutations\": 10,\n",
        "                                   \"optimizer\": 'adam',\n",
        "                                   \"regularization_weight\": 1e-05,\n",
        "                                   \"sigma_alpha\": 1.0,\n",
        "                                   \"sigma_beta\": 1.0,\n",
        "                                   \"train_iteration\": 5000})\n",
        "\n",
        "inference_args = easydict.EasyDict({\"batchsize\": 100,\n",
        "                                    \"look_ahead\": 1,\n",
        "                                    \"test_iteration\": 2,\n",
        "                                    \"beam_size\": 10})"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nzN3KqaC67z"
      },
      "source": [
        "## UISRNN모델 학습\n",
        "- 앞서 구성한 argument를 사용해 모델을 구성\n",
        "- 데이터를 입력해 모델을 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbcUrKjUC6-0",
        "outputId": "365f05c0-ced7-4be9-fd85-f2ff5b05ebcf"
      },
      "source": [
        "model = uisrnn.UISRNN(model_args)\n",
        "\n",
        "model.fit(train_sequence, train_cluster_id, training_args)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: transition_bias cannot be correctly estimated from a concatenated sequence; train_sequences will be treated as a single sequence. This can lead to inaccurate estimation of transition_bias. Please, consider estimating transition_bias before concatenating the sequences and passing it as argument.\n",
            "Iter: 0  \tTraining Loss: -282.8517    \n",
            "    Negative Log Likelihood: 6.3097\tSigma2 Prior: -289.1620\tRegularization: 0.0006\n",
            "Iter: 10  \tTraining Loss: -298.2873    \n",
            "    Negative Log Likelihood: 5.6249\tSigma2 Prior: -303.9128\tRegularization: 0.0006\n",
            "Iter: 20  \tTraining Loss: -311.7488    \n",
            "    Negative Log Likelihood: 6.2804\tSigma2 Prior: -318.0299\tRegularization: 0.0006\n",
            "Iter: 30  \tTraining Loss: -328.1163    \n",
            "    Negative Log Likelihood: 7.0495\tSigma2 Prior: -335.1664\tRegularization: 0.0006\n",
            "Iter: 40  \tTraining Loss: -343.1385    \n",
            "    Negative Log Likelihood: 8.4133\tSigma2 Prior: -351.5525\tRegularization: 0.0006\n",
            "Iter: 50  \tTraining Loss: -365.6828    \n",
            "    Negative Log Likelihood: 10.2739\tSigma2 Prior: -375.9574\tRegularization: 0.0007\n",
            "Iter: 60  \tTraining Loss: -402.6993    \n",
            "    Negative Log Likelihood: 13.8049\tSigma2 Prior: -416.5049\tRegularization: 0.0007\n",
            "Iter: 70  \tTraining Loss: -439.7920    \n",
            "    Negative Log Likelihood: 22.8929\tSigma2 Prior: -462.6856\tRegularization: 0.0007\n",
            "Iter: 80  \tTraining Loss: -459.1430    \n",
            "    Negative Log Likelihood: 64.1377\tSigma2 Prior: -523.2814\tRegularization: 0.0007\n",
            "Iter: 90  \tTraining Loss: -497.5330    \n",
            "    Negative Log Likelihood: 43.5839\tSigma2 Prior: -541.1176\tRegularization: 0.0007\n",
            "Iter: 100  \tTraining Loss: -477.9539    \n",
            "    Negative Log Likelihood: 45.6849\tSigma2 Prior: -523.6395\tRegularization: 0.0007\n",
            "Iter: 110  \tTraining Loss: -488.9244    \n",
            "    Negative Log Likelihood: 49.7813\tSigma2 Prior: -538.7064\tRegularization: 0.0007\n",
            "Iter: 120  \tTraining Loss: -480.2426    \n",
            "    Negative Log Likelihood: 39.9132\tSigma2 Prior: -520.1566\tRegularization: 0.0007\n",
            "Iter: 130  \tTraining Loss: -429.1198    \n",
            "    Negative Log Likelihood: 46.9779\tSigma2 Prior: -476.0985\tRegularization: 0.0008\n",
            "Iter: 140  \tTraining Loss: -489.8092    \n",
            "    Negative Log Likelihood: 44.6990\tSigma2 Prior: -534.5090\tRegularization: 0.0008\n",
            "Iter: 150  \tTraining Loss: -504.1889    \n",
            "    Negative Log Likelihood: 46.2204\tSigma2 Prior: -550.4101\tRegularization: 0.0008\n",
            "Iter: 160  \tTraining Loss: -480.5772    \n",
            "    Negative Log Likelihood: 39.5929\tSigma2 Prior: -520.1709\tRegularization: 0.0008\n",
            "Iter: 170  \tTraining Loss: -485.7969    \n",
            "    Negative Log Likelihood: 40.3810\tSigma2 Prior: -526.1787\tRegularization: 0.0008\n",
            "Iter: 180  \tTraining Loss: -445.7935    \n",
            "    Negative Log Likelihood: 44.5237\tSigma2 Prior: -490.3181\tRegularization: 0.0008\n",
            "Iter: 190  \tTraining Loss: -494.9794    \n",
            "    Negative Log Likelihood: 39.0032\tSigma2 Prior: -533.9835\tRegularization: 0.0008\n",
            "Iter: 200  \tTraining Loss: -505.2242    \n",
            "    Negative Log Likelihood: 46.4188\tSigma2 Prior: -551.6439\tRegularization: 0.0009\n",
            "Iter: 210  \tTraining Loss: -493.5385    \n",
            "    Negative Log Likelihood: 42.1589\tSigma2 Prior: -535.6982\tRegularization: 0.0009\n",
            "Iter: 220  \tTraining Loss: -497.4857    \n",
            "    Negative Log Likelihood: 43.2158\tSigma2 Prior: -540.7024\tRegularization: 0.0009\n",
            "Iter: 230  \tTraining Loss: -490.4846    \n",
            "    Negative Log Likelihood: 45.0949\tSigma2 Prior: -535.5804\tRegularization: 0.0009\n",
            "Iter: 240  \tTraining Loss: -492.4084    \n",
            "    Negative Log Likelihood: 40.8201\tSigma2 Prior: -533.2295\tRegularization: 0.0009\n",
            "Iter: 250  \tTraining Loss: -438.5207    \n",
            "    Negative Log Likelihood: 49.8709\tSigma2 Prior: -488.3925\tRegularization: 0.0009\n",
            "Iter: 260  \tTraining Loss: -480.3224    \n",
            "    Negative Log Likelihood: 37.3182\tSigma2 Prior: -517.6415\tRegularization: 0.0010\n",
            "Iter: 270  \tTraining Loss: -458.6464    \n",
            "    Negative Log Likelihood: 38.7884\tSigma2 Prior: -497.4358\tRegularization: 0.0010\n",
            "Iter: 280  \tTraining Loss: -481.3901    \n",
            "    Negative Log Likelihood: 42.9247\tSigma2 Prior: -524.3157\tRegularization: 0.0010\n",
            "Iter: 290  \tTraining Loss: -468.9560    \n",
            "    Negative Log Likelihood: 39.5847\tSigma2 Prior: -508.5416\tRegularization: 0.0010\n",
            "Iter: 300  \tTraining Loss: -443.8304    \n",
            "    Negative Log Likelihood: 43.7481\tSigma2 Prior: -487.5795\tRegularization: 0.0010\n",
            "Iter: 310  \tTraining Loss: -491.4132    \n",
            "    Negative Log Likelihood: 44.1786\tSigma2 Prior: -535.5928\tRegularization: 0.0010\n",
            "Iter: 320  \tTraining Loss: -491.8715    \n",
            "    Negative Log Likelihood: 44.1110\tSigma2 Prior: -535.9835\tRegularization: 0.0010\n",
            "Iter: 330  \tTraining Loss: -488.7625    \n",
            "    Negative Log Likelihood: 39.3282\tSigma2 Prior: -528.0918\tRegularization: 0.0011\n",
            "Iter: 340  \tTraining Loss: -502.5912    \n",
            "    Negative Log Likelihood: 40.0722\tSigma2 Prior: -542.6644\tRegularization: 0.0011\n",
            "Iter: 350  \tTraining Loss: -478.7775    \n",
            "    Negative Log Likelihood: 41.3011\tSigma2 Prior: -520.0797\tRegularization: 0.0011\n",
            "Iter: 360  \tTraining Loss: -505.0764    \n",
            "    Negative Log Likelihood: 37.8370\tSigma2 Prior: -542.9146\tRegularization: 0.0011\n",
            "Iter: 370  \tTraining Loss: -476.9755    \n",
            "    Negative Log Likelihood: 34.4910\tSigma2 Prior: -511.4676\tRegularization: 0.0011\n",
            "Iter: 380  \tTraining Loss: -474.4017    \n",
            "    Negative Log Likelihood: 42.7178\tSigma2 Prior: -517.1207\tRegularization: 0.0011\n",
            "Iter: 390  \tTraining Loss: -488.8533    \n",
            "    Negative Log Likelihood: 39.1670\tSigma2 Prior: -528.0215\tRegularization: 0.0011\n",
            "Iter: 400  \tTraining Loss: -518.6650    \n",
            "    Negative Log Likelihood: 35.5142\tSigma2 Prior: -554.1804\tRegularization: 0.0011\n",
            "Iter: 410  \tTraining Loss: -453.4682    \n",
            "    Negative Log Likelihood: 49.1926\tSigma2 Prior: -502.6619\tRegularization: 0.0011\n",
            "Iter: 420  \tTraining Loss: -481.4709    \n",
            "    Negative Log Likelihood: 35.8738\tSigma2 Prior: -517.3458\tRegularization: 0.0012\n",
            "Iter: 430  \tTraining Loss: -481.2243    \n",
            "    Negative Log Likelihood: 40.3529\tSigma2 Prior: -521.5784\tRegularization: 0.0012\n",
            "Iter: 440  \tTraining Loss: -459.9345    \n",
            "    Negative Log Likelihood: 39.2830\tSigma2 Prior: -499.2187\tRegularization: 0.0012\n",
            "Iter: 450  \tTraining Loss: -499.3139    \n",
            "    Negative Log Likelihood: 37.2796\tSigma2 Prior: -536.5947\tRegularization: 0.0012\n",
            "Iter: 460  \tTraining Loss: -447.0677    \n",
            "    Negative Log Likelihood: 39.5485\tSigma2 Prior: -486.6174\tRegularization: 0.0012\n",
            "Iter: 470  \tTraining Loss: -502.3026    \n",
            "    Negative Log Likelihood: 35.4917\tSigma2 Prior: -537.7954\tRegularization: 0.0012\n",
            "Iter: 480  \tTraining Loss: -486.4604    \n",
            "    Negative Log Likelihood: 35.6049\tSigma2 Prior: -522.0665\tRegularization: 0.0012\n",
            "Iter: 490  \tTraining Loss: -491.3526    \n",
            "    Negative Log Likelihood: 39.7208\tSigma2 Prior: -531.0746\tRegularization: 0.0012\n",
            "Iter: 500  \tTraining Loss: -489.8914    \n",
            "    Negative Log Likelihood: 38.0669\tSigma2 Prior: -527.9595\tRegularization: 0.0012\n",
            "Iter: 510  \tTraining Loss: -493.5599    \n",
            "    Negative Log Likelihood: 33.9296\tSigma2 Prior: -527.4907\tRegularization: 0.0012\n",
            "Iter: 520  \tTraining Loss: -449.3644    \n",
            "    Negative Log Likelihood: 43.1073\tSigma2 Prior: -492.4730\tRegularization: 0.0012\n",
            "Iter: 530  \tTraining Loss: -465.4594    \n",
            "    Negative Log Likelihood: 34.9023\tSigma2 Prior: -500.3629\tRegularization: 0.0012\n",
            "Iter: 540  \tTraining Loss: -489.5702    \n",
            "    Negative Log Likelihood: 42.4337\tSigma2 Prior: -532.0052\tRegularization: 0.0012\n",
            "Iter: 550  \tTraining Loss: -485.1444    \n",
            "    Negative Log Likelihood: 32.9181\tSigma2 Prior: -518.0638\tRegularization: 0.0013\n",
            "Iter: 560  \tTraining Loss: -482.7775    \n",
            "    Negative Log Likelihood: 40.1901\tSigma2 Prior: -522.9688\tRegularization: 0.0013\n",
            "Iter: 570  \tTraining Loss: -453.2288    \n",
            "    Negative Log Likelihood: 38.6593\tSigma2 Prior: -491.8893\tRegularization: 0.0013\n",
            "Iter: 580  \tTraining Loss: -488.8210    \n",
            "    Negative Log Likelihood: 40.7524\tSigma2 Prior: -529.5747\tRegularization: 0.0013\n",
            "Iter: 590  \tTraining Loss: -487.9980    \n",
            "    Negative Log Likelihood: 40.6168\tSigma2 Prior: -528.6161\tRegularization: 0.0013\n",
            "Iter: 600  \tTraining Loss: -471.9014    \n",
            "    Negative Log Likelihood: 38.7107\tSigma2 Prior: -510.6133\tRegularization: 0.0013\n",
            "Iter: 610  \tTraining Loss: -493.9746    \n",
            "    Negative Log Likelihood: 37.3075\tSigma2 Prior: -531.2833\tRegularization: 0.0013\n",
            "Iter: 620  \tTraining Loss: -477.7545    \n",
            "    Negative Log Likelihood: 39.8394\tSigma2 Prior: -517.5952\tRegularization: 0.0013\n",
            "Iter: 630  \tTraining Loss: -486.3952    \n",
            "    Negative Log Likelihood: 29.9233\tSigma2 Prior: -516.3198\tRegularization: 0.0013\n",
            "Iter: 640  \tTraining Loss: -448.3591    \n",
            "    Negative Log Likelihood: 37.8557\tSigma2 Prior: -486.2161\tRegularization: 0.0013\n",
            "Iter: 650  \tTraining Loss: -483.1045    \n",
            "    Negative Log Likelihood: 36.7716\tSigma2 Prior: -519.8774\tRegularization: 0.0013\n",
            "Iter: 660  \tTraining Loss: -485.3611    \n",
            "    Negative Log Likelihood: 38.9285\tSigma2 Prior: -524.2910\tRegularization: 0.0013\n",
            "Iter: 670  \tTraining Loss: -466.1117    \n",
            "    Negative Log Likelihood: 35.3625\tSigma2 Prior: -501.4755\tRegularization: 0.0013\n",
            "Iter: 680  \tTraining Loss: -503.3406    \n",
            "    Negative Log Likelihood: 33.3111\tSigma2 Prior: -536.6530\tRegularization: 0.0013\n",
            "Iter: 690  \tTraining Loss: -488.0070    \n",
            "    Negative Log Likelihood: 42.4749\tSigma2 Prior: -530.4832\tRegularization: 0.0013\n",
            "Iter: 700  \tTraining Loss: -493.9410    \n",
            "    Negative Log Likelihood: 37.4703\tSigma2 Prior: -531.4126\tRegularization: 0.0013\n",
            "Iter: 710  \tTraining Loss: -438.4655    \n",
            "    Negative Log Likelihood: 37.6186\tSigma2 Prior: -476.0854\tRegularization: 0.0013\n",
            "Iter: 720  \tTraining Loss: -504.4338    \n",
            "    Negative Log Likelihood: 34.8817\tSigma2 Prior: -539.3168\tRegularization: 0.0013\n",
            "Iter: 730  \tTraining Loss: -497.8645    \n",
            "    Negative Log Likelihood: 36.6114\tSigma2 Prior: -534.4772\tRegularization: 0.0014\n",
            "Iter: 740  \tTraining Loss: -461.4269    \n",
            "    Negative Log Likelihood: 36.3795\tSigma2 Prior: -497.8077\tRegularization: 0.0014\n",
            "Iter: 750  \tTraining Loss: -475.3191    \n",
            "    Negative Log Likelihood: 33.5406\tSigma2 Prior: -508.8610\tRegularization: 0.0014\n",
            "Iter: 760  \tTraining Loss: -466.6785    \n",
            "    Negative Log Likelihood: 39.3907\tSigma2 Prior: -506.0705\tRegularization: 0.0014\n",
            "Iter: 770  \tTraining Loss: -514.7106    \n",
            "    Negative Log Likelihood: 41.6982\tSigma2 Prior: -556.4103\tRegularization: 0.0014\n",
            "Iter: 780  \tTraining Loss: -502.8593    \n",
            "    Negative Log Likelihood: 31.5782\tSigma2 Prior: -534.4389\tRegularization: 0.0014\n",
            "Iter: 790  \tTraining Loss: -495.5996    \n",
            "    Negative Log Likelihood: 41.0783\tSigma2 Prior: -536.6793\tRegularization: 0.0014\n",
            "Iter: 800  \tTraining Loss: -514.2300    \n",
            "    Negative Log Likelihood: 36.2797\tSigma2 Prior: -550.5111\tRegularization: 0.0014\n",
            "Iter: 810  \tTraining Loss: -499.4181    \n",
            "    Negative Log Likelihood: 40.9532\tSigma2 Prior: -540.3727\tRegularization: 0.0014\n",
            "Iter: 820  \tTraining Loss: -491.8617    \n",
            "    Negative Log Likelihood: 39.4227\tSigma2 Prior: -531.2858\tRegularization: 0.0014\n",
            "Iter: 830  \tTraining Loss: -487.9672    \n",
            "    Negative Log Likelihood: 36.2835\tSigma2 Prior: -524.2521\tRegularization: 0.0014\n",
            "Iter: 840  \tTraining Loss: -492.6816    \n",
            "    Negative Log Likelihood: 40.4908\tSigma2 Prior: -533.1739\tRegularization: 0.0014\n",
            "Iter: 850  \tTraining Loss: -446.1528    \n",
            "    Negative Log Likelihood: 35.5579\tSigma2 Prior: -481.7121\tRegularization: 0.0014\n",
            "Iter: 860  \tTraining Loss: -499.3116    \n",
            "    Negative Log Likelihood: 39.1821\tSigma2 Prior: -538.4951\tRegularization: 0.0014\n",
            "Iter: 870  \tTraining Loss: -530.8072    \n",
            "    Negative Log Likelihood: 39.3263\tSigma2 Prior: -570.1349\tRegularization: 0.0014\n",
            "Iter: 880  \tTraining Loss: -508.9324    \n",
            "    Negative Log Likelihood: 36.3852\tSigma2 Prior: -545.3191\tRegularization: 0.0014\n",
            "Iter: 890  \tTraining Loss: -500.8236    \n",
            "    Negative Log Likelihood: 37.6169\tSigma2 Prior: -538.4419\tRegularization: 0.0014\n",
            "Iter: 900  \tTraining Loss: -477.9564    \n",
            "    Negative Log Likelihood: 34.7080\tSigma2 Prior: -512.6658\tRegularization: 0.0014\n",
            "Iter: 910  \tTraining Loss: -494.0334    \n",
            "    Negative Log Likelihood: 37.5802\tSigma2 Prior: -531.6150\tRegularization: 0.0014\n",
            "Iter: 920  \tTraining Loss: -474.1078    \n",
            "    Negative Log Likelihood: 36.5312\tSigma2 Prior: -510.6404\tRegularization: 0.0014\n",
            "Iter: 930  \tTraining Loss: -465.4158    \n",
            "    Negative Log Likelihood: 39.3388\tSigma2 Prior: -504.7560\tRegularization: 0.0014\n",
            "Iter: 940  \tTraining Loss: -457.1789    \n",
            "    Negative Log Likelihood: 40.2944\tSigma2 Prior: -497.4748\tRegularization: 0.0015\n",
            "Iter: 950  \tTraining Loss: -498.8177    \n",
            "    Negative Log Likelihood: 31.8310\tSigma2 Prior: -530.6503\tRegularization: 0.0015\n",
            "Iter: 960  \tTraining Loss: -529.2754    \n",
            "    Negative Log Likelihood: 35.7626\tSigma2 Prior: -565.0394\tRegularization: 0.0015\n",
            "Iter: 970  \tTraining Loss: -478.9556    \n",
            "    Negative Log Likelihood: 39.0920\tSigma2 Prior: -518.0490\tRegularization: 0.0015\n",
            "Iter: 980  \tTraining Loss: -504.3577    \n",
            "    Negative Log Likelihood: 32.3478\tSigma2 Prior: -536.7070\tRegularization: 0.0015\n",
            "Iter: 990  \tTraining Loss: -510.9354    \n",
            "    Negative Log Likelihood: 33.8751\tSigma2 Prior: -544.8120\tRegularization: 0.0015\n",
            "Iter: 1000  \tTraining Loss: -474.8327    \n",
            "    Negative Log Likelihood: 33.6302\tSigma2 Prior: -508.4644\tRegularization: 0.0015\n",
            "Iter: 1010  \tTraining Loss: -435.3263    \n",
            "    Negative Log Likelihood: 35.2366\tSigma2 Prior: -470.5644\tRegularization: 0.0015\n",
            "Iter: 1020  \tTraining Loss: -482.9370    \n",
            "    Negative Log Likelihood: 33.8782\tSigma2 Prior: -516.8167\tRegularization: 0.0015\n",
            "Iter: 1030  \tTraining Loss: -519.2446    \n",
            "    Negative Log Likelihood: 40.8747\tSigma2 Prior: -560.1207\tRegularization: 0.0015\n",
            "Iter: 1040  \tTraining Loss: -461.9618    \n",
            "    Negative Log Likelihood: 36.1247\tSigma2 Prior: -498.0881\tRegularization: 0.0015\n",
            "Iter: 1050  \tTraining Loss: -513.0419    \n",
            "    Negative Log Likelihood: 36.3650\tSigma2 Prior: -549.4084\tRegularization: 0.0015\n",
            "Iter: 1060  \tTraining Loss: -508.2121    \n",
            "    Negative Log Likelihood: 41.1774\tSigma2 Prior: -549.3909\tRegularization: 0.0015\n",
            "Iter: 1070  \tTraining Loss: -463.9693    \n",
            "    Negative Log Likelihood: 30.0831\tSigma2 Prior: -494.0540\tRegularization: 0.0015\n",
            "Iter: 1080  \tTraining Loss: -490.0414    \n",
            "    Negative Log Likelihood: 40.6554\tSigma2 Prior: -530.6982\tRegularization: 0.0015\n",
            "Iter: 1090  \tTraining Loss: -506.7632    \n",
            "    Negative Log Likelihood: 32.6976\tSigma2 Prior: -539.4623\tRegularization: 0.0015\n",
            "Iter: 1100  \tTraining Loss: -496.6110    \n",
            "    Negative Log Likelihood: 36.7379\tSigma2 Prior: -533.3504\tRegularization: 0.0015\n",
            "Iter: 1110  \tTraining Loss: -501.4774    \n",
            "    Negative Log Likelihood: 35.8384\tSigma2 Prior: -537.3173\tRegularization: 0.0015\n",
            "Iter: 1120  \tTraining Loss: -492.3074    \n",
            "    Negative Log Likelihood: 38.2286\tSigma2 Prior: -530.5375\tRegularization: 0.0015\n",
            "Iter: 1130  \tTraining Loss: -501.0205    \n",
            "    Negative Log Likelihood: 32.9109\tSigma2 Prior: -533.9330\tRegularization: 0.0015\n",
            "Iter: 1140  \tTraining Loss: -512.8270    \n",
            "    Negative Log Likelihood: 38.7869\tSigma2 Prior: -551.6154\tRegularization: 0.0015\n",
            "Iter: 1150  \tTraining Loss: -477.4539    \n",
            "    Negative Log Likelihood: 35.4528\tSigma2 Prior: -512.9083\tRegularization: 0.0015\n",
            "Iter: 1160  \tTraining Loss: -478.3463    \n",
            "    Negative Log Likelihood: 37.1954\tSigma2 Prior: -515.5433\tRegularization: 0.0015\n",
            "Iter: 1170  \tTraining Loss: -467.4900    \n",
            "    Negative Log Likelihood: 31.8613\tSigma2 Prior: -499.3528\tRegularization: 0.0015\n",
            "Iter: 1180  \tTraining Loss: -500.1779    \n",
            "    Negative Log Likelihood: 35.0696\tSigma2 Prior: -535.2491\tRegularization: 0.0015\n",
            "Iter: 1190  \tTraining Loss: -487.7598    \n",
            "    Negative Log Likelihood: 35.5639\tSigma2 Prior: -523.3253\tRegularization: 0.0016\n",
            "Iter: 1200  \tTraining Loss: -479.5073    \n",
            "    Negative Log Likelihood: 33.6367\tSigma2 Prior: -513.1455\tRegularization: 0.0016\n",
            "Iter: 1210  \tTraining Loss: -470.4450    \n",
            "    Negative Log Likelihood: 44.8114\tSigma2 Prior: -515.2579\tRegularization: 0.0016\n",
            "Iter: 1220  \tTraining Loss: -504.3218    \n",
            "    Negative Log Likelihood: 32.1202\tSigma2 Prior: -536.4436\tRegularization: 0.0016\n",
            "Iter: 1230  \tTraining Loss: -485.3840    \n",
            "    Negative Log Likelihood: 33.3121\tSigma2 Prior: -518.6977\tRegularization: 0.0016\n",
            "Iter: 1240  \tTraining Loss: -494.9311    \n",
            "    Negative Log Likelihood: 35.7982\tSigma2 Prior: -530.7309\tRegularization: 0.0016\n",
            "Iter: 1250  \tTraining Loss: -486.5722    \n",
            "    Negative Log Likelihood: 34.2821\tSigma2 Prior: -520.8558\tRegularization: 0.0016\n",
            "Iter: 1260  \tTraining Loss: -516.8049    \n",
            "    Negative Log Likelihood: 38.0378\tSigma2 Prior: -554.8444\tRegularization: 0.0016\n",
            "Iter: 1270  \tTraining Loss: -498.7094    \n",
            "    Negative Log Likelihood: 36.7502\tSigma2 Prior: -535.4612\tRegularization: 0.0016\n",
            "Iter: 1280  \tTraining Loss: -514.9983    \n",
            "    Negative Log Likelihood: 31.3879\tSigma2 Prior: -546.3878\tRegularization: 0.0016\n",
            "Iter: 1290  \tTraining Loss: -481.6000    \n",
            "    Negative Log Likelihood: 41.9366\tSigma2 Prior: -523.5383\tRegularization: 0.0016\n",
            "Iter: 1300  \tTraining Loss: -492.8233    \n",
            "    Negative Log Likelihood: 30.3609\tSigma2 Prior: -523.1857\tRegularization: 0.0016\n",
            "Iter: 1310  \tTraining Loss: -480.7932    \n",
            "    Negative Log Likelihood: 38.0358\tSigma2 Prior: -518.8306\tRegularization: 0.0016\n",
            "Iter: 1320  \tTraining Loss: -509.5682    \n",
            "    Negative Log Likelihood: 31.3715\tSigma2 Prior: -540.9413\tRegularization: 0.0016\n",
            "Iter: 1330  \tTraining Loss: -513.1243    \n",
            "    Negative Log Likelihood: 33.6734\tSigma2 Prior: -546.7993\tRegularization: 0.0016\n",
            "Iter: 1340  \tTraining Loss: -490.6172    \n",
            "    Negative Log Likelihood: 37.1563\tSigma2 Prior: -527.7750\tRegularization: 0.0016\n",
            "Iter: 1350  \tTraining Loss: -503.1040    \n",
            "    Negative Log Likelihood: 35.3509\tSigma2 Prior: -538.4565\tRegularization: 0.0016\n",
            "Iter: 1360  \tTraining Loss: -504.6691    \n",
            "    Negative Log Likelihood: 36.2941\tSigma2 Prior: -540.9648\tRegularization: 0.0016\n",
            "Iter: 1370  \tTraining Loss: -504.0341    \n",
            "    Negative Log Likelihood: 33.6278\tSigma2 Prior: -537.6635\tRegularization: 0.0016\n",
            "Iter: 1380  \tTraining Loss: -509.5070    \n",
            "    Negative Log Likelihood: 36.3574\tSigma2 Prior: -545.8661\tRegularization: 0.0016\n",
            "Iter: 1390  \tTraining Loss: -503.7682    \n",
            "    Negative Log Likelihood: 31.5200\tSigma2 Prior: -535.2899\tRegularization: 0.0016\n",
            "Iter: 1400  \tTraining Loss: -494.7967    \n",
            "    Negative Log Likelihood: 38.1286\tSigma2 Prior: -532.9269\tRegularization: 0.0016\n",
            "Iter: 1410  \tTraining Loss: -489.7396    \n",
            "    Negative Log Likelihood: 38.1273\tSigma2 Prior: -527.8685\tRegularization: 0.0016\n",
            "Iter: 1420  \tTraining Loss: -526.9901    \n",
            "    Negative Log Likelihood: 33.5015\tSigma2 Prior: -560.4932\tRegularization: 0.0016\n",
            "Iter: 1430  \tTraining Loss: -510.6429    \n",
            "    Negative Log Likelihood: 35.9620\tSigma2 Prior: -546.6064\tRegularization: 0.0016\n",
            "Iter: 1440  \tTraining Loss: -502.4365    \n",
            "    Negative Log Likelihood: 36.1280\tSigma2 Prior: -538.5662\tRegularization: 0.0016\n",
            "Iter: 1450  \tTraining Loss: -505.4388    \n",
            "    Negative Log Likelihood: 42.4603\tSigma2 Prior: -547.9008\tRegularization: 0.0016\n",
            "Iter: 1460  \tTraining Loss: -487.3778    \n",
            "    Negative Log Likelihood: 28.4917\tSigma2 Prior: -515.8712\tRegularization: 0.0016\n",
            "Iter: 1470  \tTraining Loss: -511.7646    \n",
            "    Negative Log Likelihood: 31.8435\tSigma2 Prior: -543.6097\tRegularization: 0.0016\n",
            "Iter: 1480  \tTraining Loss: -478.4420    \n",
            "    Negative Log Likelihood: 37.8199\tSigma2 Prior: -516.2635\tRegularization: 0.0017\n",
            "Iter: 1490  \tTraining Loss: -500.7154    \n",
            "    Negative Log Likelihood: 35.0631\tSigma2 Prior: -535.7802\tRegularization: 0.0017\n",
            "Iter: 1500  \tTraining Loss: -508.2826    \n",
            "    Negative Log Likelihood: 33.7094\tSigma2 Prior: -541.9937\tRegularization: 0.0017\n",
            "Iter: 1510  \tTraining Loss: -520.1669    \n",
            "    Negative Log Likelihood: 35.8019\tSigma2 Prior: -555.9705\tRegularization: 0.0017\n",
            "Iter: 1520  \tTraining Loss: -494.4232    \n",
            "    Negative Log Likelihood: 31.5206\tSigma2 Prior: -525.9454\tRegularization: 0.0017\n",
            "Iter: 1530  \tTraining Loss: -496.6416    \n",
            "    Negative Log Likelihood: 32.2845\tSigma2 Prior: -528.9279\tRegularization: 0.0017\n",
            "Iter: 1540  \tTraining Loss: -503.1930    \n",
            "    Negative Log Likelihood: 38.8097\tSigma2 Prior: -542.0044\tRegularization: 0.0017\n",
            "Iter: 1550  \tTraining Loss: -471.0020    \n",
            "    Negative Log Likelihood: 35.1766\tSigma2 Prior: -506.1803\tRegularization: 0.0017\n",
            "Iter: 1560  \tTraining Loss: -509.7229    \n",
            "    Negative Log Likelihood: 36.8948\tSigma2 Prior: -546.6194\tRegularization: 0.0017\n",
            "Iter: 1570  \tTraining Loss: -479.0126    \n",
            "    Negative Log Likelihood: 34.9590\tSigma2 Prior: -513.9733\tRegularization: 0.0017\n",
            "Iter: 1580  \tTraining Loss: -490.4890    \n",
            "    Negative Log Likelihood: 36.5192\tSigma2 Prior: -527.0099\tRegularization: 0.0017\n",
            "Iter: 1590  \tTraining Loss: -506.6215    \n",
            "    Negative Log Likelihood: 36.4352\tSigma2 Prior: -543.0584\tRegularization: 0.0017\n",
            "Iter: 1600  \tTraining Loss: -468.5316    \n",
            "    Negative Log Likelihood: 36.9478\tSigma2 Prior: -505.4811\tRegularization: 0.0017\n",
            "Iter: 1610  \tTraining Loss: -504.2277    \n",
            "    Negative Log Likelihood: 34.2547\tSigma2 Prior: -538.4841\tRegularization: 0.0017\n",
            "Iter: 1620  \tTraining Loss: -486.7531    \n",
            "    Negative Log Likelihood: 38.6784\tSigma2 Prior: -525.4332\tRegularization: 0.0017\n",
            "Iter: 1630  \tTraining Loss: -453.6047    \n",
            "    Negative Log Likelihood: 36.2749\tSigma2 Prior: -489.8813\tRegularization: 0.0017\n",
            "Iter: 1640  \tTraining Loss: -481.6194    \n",
            "    Negative Log Likelihood: 36.8770\tSigma2 Prior: -518.4981\tRegularization: 0.0017\n",
            "Iter: 1650  \tTraining Loss: -501.0591    \n",
            "    Negative Log Likelihood: 34.4598\tSigma2 Prior: -535.5206\tRegularization: 0.0017\n",
            "Iter: 1660  \tTraining Loss: -507.1201    \n",
            "    Negative Log Likelihood: 29.0464\tSigma2 Prior: -536.1682\tRegularization: 0.0017\n",
            "Iter: 1670  \tTraining Loss: -514.2989    \n",
            "    Negative Log Likelihood: 34.8718\tSigma2 Prior: -549.1724\tRegularization: 0.0017\n",
            "Iter: 1680  \tTraining Loss: -488.7804    \n",
            "    Negative Log Likelihood: 34.4065\tSigma2 Prior: -523.1886\tRegularization: 0.0017\n",
            "Iter: 1690  \tTraining Loss: -466.1012    \n",
            "    Negative Log Likelihood: 34.5006\tSigma2 Prior: -500.6035\tRegularization: 0.0017\n",
            "Iter: 1700  \tTraining Loss: -492.9848    \n",
            "    Negative Log Likelihood: 37.3640\tSigma2 Prior: -530.3505\tRegularization: 0.0017\n",
            "Iter: 1710  \tTraining Loss: -500.0067    \n",
            "    Negative Log Likelihood: 36.7659\tSigma2 Prior: -536.7743\tRegularization: 0.0017\n",
            "Iter: 1720  \tTraining Loss: -481.8542    \n",
            "    Negative Log Likelihood: 37.3777\tSigma2 Prior: -519.2336\tRegularization: 0.0017\n",
            "Iter: 1730  \tTraining Loss: -483.2746    \n",
            "    Negative Log Likelihood: 32.3326\tSigma2 Prior: -515.6089\tRegularization: 0.0017\n",
            "Iter: 1740  \tTraining Loss: -494.2541    \n",
            "    Negative Log Likelihood: 35.2365\tSigma2 Prior: -529.4924\tRegularization: 0.0017\n",
            "Iter: 1750  \tTraining Loss: -497.2355    \n",
            "    Negative Log Likelihood: 31.9612\tSigma2 Prior: -529.1985\tRegularization: 0.0017\n",
            "Iter: 1760  \tTraining Loss: -432.8333    \n",
            "    Negative Log Likelihood: 41.9854\tSigma2 Prior: -474.8204\tRegularization: 0.0017\n",
            "Iter: 1770  \tTraining Loss: -485.8070    \n",
            "    Negative Log Likelihood: 32.5548\tSigma2 Prior: -518.3636\tRegularization: 0.0017\n",
            "Iter: 1780  \tTraining Loss: -482.7620    \n",
            "    Negative Log Likelihood: 39.5887\tSigma2 Prior: -522.3524\tRegularization: 0.0017\n",
            "Iter: 1790  \tTraining Loss: -482.9850    \n",
            "    Negative Log Likelihood: 30.3101\tSigma2 Prior: -513.2968\tRegularization: 0.0017\n",
            "Iter: 1800  \tTraining Loss: -498.6483    \n",
            "    Negative Log Likelihood: 36.7732\tSigma2 Prior: -535.4232\tRegularization: 0.0018\n",
            "Iter: 1810  \tTraining Loss: -504.3890    \n",
            "    Negative Log Likelihood: 36.5356\tSigma2 Prior: -540.9263\tRegularization: 0.0018\n",
            "Iter: 1820  \tTraining Loss: -509.0916    \n",
            "    Negative Log Likelihood: 31.1469\tSigma2 Prior: -540.2403\tRegularization: 0.0018\n",
            "Iter: 1830  \tTraining Loss: -500.3065    \n",
            "    Negative Log Likelihood: 36.8728\tSigma2 Prior: -537.1811\tRegularization: 0.0018\n",
            "Iter: 1840  \tTraining Loss: -494.4842    \n",
            "    Negative Log Likelihood: 29.7760\tSigma2 Prior: -524.2620\tRegularization: 0.0018\n",
            "Iter: 1850  \tTraining Loss: -492.1418    \n",
            "    Negative Log Likelihood: 38.7890\tSigma2 Prior: -530.9325\tRegularization: 0.0018\n",
            "Iter: 1860  \tTraining Loss: -485.1823    \n",
            "    Negative Log Likelihood: 37.0212\tSigma2 Prior: -522.2053\tRegularization: 0.0018\n",
            "Iter: 1870  \tTraining Loss: -511.0846    \n",
            "    Negative Log Likelihood: 33.3489\tSigma2 Prior: -544.4353\tRegularization: 0.0018\n",
            "Iter: 1880  \tTraining Loss: -491.8778    \n",
            "    Negative Log Likelihood: 37.1041\tSigma2 Prior: -528.9837\tRegularization: 0.0018\n",
            "Iter: 1890  \tTraining Loss: -474.0699    \n",
            "    Negative Log Likelihood: 30.7479\tSigma2 Prior: -504.8196\tRegularization: 0.0018\n",
            "Iter: 1900  \tTraining Loss: -501.2974    \n",
            "    Negative Log Likelihood: 37.8312\tSigma2 Prior: -539.1304\tRegularization: 0.0018\n",
            "Iter: 1910  \tTraining Loss: -512.5441    \n",
            "    Negative Log Likelihood: 35.3612\tSigma2 Prior: -547.9071\tRegularization: 0.0018\n",
            "Iter: 1920  \tTraining Loss: -505.1042    \n",
            "    Negative Log Likelihood: 38.1100\tSigma2 Prior: -543.2159\tRegularization: 0.0018\n",
            "Iter: 1930  \tTraining Loss: -504.3227    \n",
            "    Negative Log Likelihood: 32.0451\tSigma2 Prior: -536.3696\tRegularization: 0.0018\n",
            "Iter: 1940  \tTraining Loss: -511.7931    \n",
            "    Negative Log Likelihood: 35.8306\tSigma2 Prior: -547.6254\tRegularization: 0.0018\n",
            "Iter: 1950  \tTraining Loss: -495.1612    \n",
            "    Negative Log Likelihood: 37.0911\tSigma2 Prior: -532.2541\tRegularization: 0.0018\n",
            "Iter: 1960  \tTraining Loss: -466.4174    \n",
            "    Negative Log Likelihood: 29.6378\tSigma2 Prior: -496.0570\tRegularization: 0.0018\n",
            "Iter: 1970  \tTraining Loss: -493.2548    \n",
            "    Negative Log Likelihood: 33.4429\tSigma2 Prior: -526.6995\tRegularization: 0.0018\n",
            "Iter: 1980  \tTraining Loss: -510.9823    \n",
            "    Negative Log Likelihood: 37.4172\tSigma2 Prior: -548.4013\tRegularization: 0.0018\n",
            "Iter: 1990  \tTraining Loss: -477.4954    \n",
            "    Negative Log Likelihood: 33.6812\tSigma2 Prior: -511.1784\tRegularization: 0.0018\n",
            "Iter: 2000  \tTraining Loss: -495.9822    \n",
            "    Negative Log Likelihood: 33.8885\tSigma2 Prior: -529.8725\tRegularization: 0.0018\n",
            "Iter: 2010  \tTraining Loss: -512.6146    \n",
            "    Negative Log Likelihood: 38.2948\tSigma2 Prior: -550.9113\tRegularization: 0.0018\n",
            "Iter: 2020  \tTraining Loss: -469.8258    \n",
            "    Negative Log Likelihood: 29.7877\tSigma2 Prior: -499.6154\tRegularization: 0.0018\n",
            "Iter: 2030  \tTraining Loss: -501.9704    \n",
            "    Negative Log Likelihood: 35.1462\tSigma2 Prior: -537.1185\tRegularization: 0.0018\n",
            "Iter: 2040  \tTraining Loss: -497.3590    \n",
            "    Negative Log Likelihood: 34.3281\tSigma2 Prior: -531.6890\tRegularization: 0.0018\n",
            "Iter: 2050  \tTraining Loss: -506.9736    \n",
            "    Negative Log Likelihood: 32.9249\tSigma2 Prior: -539.9004\tRegularization: 0.0018\n",
            "Iter: 2060  \tTraining Loss: -507.9116    \n",
            "    Negative Log Likelihood: 34.0735\tSigma2 Prior: -541.9870\tRegularization: 0.0018\n",
            "Iter: 2070  \tTraining Loss: -472.4399    \n",
            "    Negative Log Likelihood: 34.3356\tSigma2 Prior: -506.7773\tRegularization: 0.0018\n",
            "Iter: 2080  \tTraining Loss: -480.1469    \n",
            "    Negative Log Likelihood: 36.0427\tSigma2 Prior: -516.1915\tRegularization: 0.0018\n",
            "Iter: 2090  \tTraining Loss: -472.7219    \n",
            "    Negative Log Likelihood: 38.9599\tSigma2 Prior: -511.6837\tRegularization: 0.0018\n",
            "Iter: 2100  \tTraining Loss: -491.9204    \n",
            "    Negative Log Likelihood: 29.6944\tSigma2 Prior: -521.6167\tRegularization: 0.0018\n",
            "Iter: 2110  \tTraining Loss: -497.0032    \n",
            "    Negative Log Likelihood: 33.2261\tSigma2 Prior: -530.2312\tRegularization: 0.0018\n",
            "Iter: 2120  \tTraining Loss: -461.6383    \n",
            "    Negative Log Likelihood: 39.2024\tSigma2 Prior: -500.8426\tRegularization: 0.0018\n",
            "Iter: 2130  \tTraining Loss: -455.3785    \n",
            "    Negative Log Likelihood: 32.1560\tSigma2 Prior: -487.5364\tRegularization: 0.0019\n",
            "Iter: 2140  \tTraining Loss: -497.4070    \n",
            "    Negative Log Likelihood: 34.6910\tSigma2 Prior: -532.0999\tRegularization: 0.0019\n",
            "Iter: 2150  \tTraining Loss: -490.8797    \n",
            "    Negative Log Likelihood: 31.0908\tSigma2 Prior: -521.9724\tRegularization: 0.0019\n",
            "Iter: 2160  \tTraining Loss: -477.7042    \n",
            "    Negative Log Likelihood: 36.0480\tSigma2 Prior: -513.7540\tRegularization: 0.0019\n",
            "Iter: 2170  \tTraining Loss: -471.3615    \n",
            "    Negative Log Likelihood: 43.4870\tSigma2 Prior: -514.8503\tRegularization: 0.0019\n",
            "Iter: 2180  \tTraining Loss: -461.4591    \n",
            "    Negative Log Likelihood: 31.0699\tSigma2 Prior: -492.5308\tRegularization: 0.0019\n",
            "Iter: 2190  \tTraining Loss: -490.2074    \n",
            "    Negative Log Likelihood: 33.3185\tSigma2 Prior: -523.5278\tRegularization: 0.0019\n",
            "Iter: 2200  \tTraining Loss: -495.5190    \n",
            "    Negative Log Likelihood: 32.6406\tSigma2 Prior: -528.1614\tRegularization: 0.0019\n",
            "Iter: 2210  \tTraining Loss: -457.7197    \n",
            "    Negative Log Likelihood: 33.6060\tSigma2 Prior: -491.3276\tRegularization: 0.0019\n",
            "Iter: 2220  \tTraining Loss: -516.2632    \n",
            "    Negative Log Likelihood: 32.2542\tSigma2 Prior: -548.5193\tRegularization: 0.0019\n",
            "Iter: 2230  \tTraining Loss: -503.5746    \n",
            "    Negative Log Likelihood: 38.8326\tSigma2 Prior: -542.4092\tRegularization: 0.0019\n",
            "Iter: 2240  \tTraining Loss: -468.3401    \n",
            "    Negative Log Likelihood: 37.1600\tSigma2 Prior: -505.5020\tRegularization: 0.0019\n",
            "Iter: 2250  \tTraining Loss: -505.2560    \n",
            "    Negative Log Likelihood: 33.0851\tSigma2 Prior: -538.3430\tRegularization: 0.0019\n",
            "Iter: 2260  \tTraining Loss: -482.0730    \n",
            "    Negative Log Likelihood: 36.9395\tSigma2 Prior: -519.0144\tRegularization: 0.0019\n",
            "Iter: 2270  \tTraining Loss: -471.1172    \n",
            "    Negative Log Likelihood: 35.6579\tSigma2 Prior: -506.7770\tRegularization: 0.0019\n",
            "Iter: 2280  \tTraining Loss: -511.4756    \n",
            "    Negative Log Likelihood: 33.9361\tSigma2 Prior: -545.4136\tRegularization: 0.0019\n",
            "Iter: 2290  \tTraining Loss: -488.5834    \n",
            "    Negative Log Likelihood: 35.2683\tSigma2 Prior: -523.8536\tRegularization: 0.0019\n",
            "Iter: 2300  \tTraining Loss: -509.5953    \n",
            "    Negative Log Likelihood: 36.6524\tSigma2 Prior: -546.2496\tRegularization: 0.0019\n",
            "Iter: 2310  \tTraining Loss: -507.0280    \n",
            "    Negative Log Likelihood: 29.0844\tSigma2 Prior: -536.1143\tRegularization: 0.0019\n",
            "Iter: 2320  \tTraining Loss: -486.6024    \n",
            "    Negative Log Likelihood: 37.2543\tSigma2 Prior: -523.8586\tRegularization: 0.0019\n",
            "Iter: 2330  \tTraining Loss: -498.4283    \n",
            "    Negative Log Likelihood: 37.5518\tSigma2 Prior: -535.9820\tRegularization: 0.0019\n",
            "Iter: 2340  \tTraining Loss: -487.7848    \n",
            "    Negative Log Likelihood: 29.7661\tSigma2 Prior: -517.5527\tRegularization: 0.0019\n",
            "Iter: 2350  \tTraining Loss: -501.3871    \n",
            "    Negative Log Likelihood: 33.9652\tSigma2 Prior: -535.3542\tRegularization: 0.0019\n",
            "Iter: 2360  \tTraining Loss: -495.8637    \n",
            "    Negative Log Likelihood: 34.5503\tSigma2 Prior: -530.4159\tRegularization: 0.0019\n",
            "Iter: 2370  \tTraining Loss: -484.2007    \n",
            "    Negative Log Likelihood: 30.1574\tSigma2 Prior: -514.3601\tRegularization: 0.0019\n",
            "Iter: 2380  \tTraining Loss: -454.2370    \n",
            "    Negative Log Likelihood: 39.8588\tSigma2 Prior: -494.0977\tRegularization: 0.0019\n",
            "Iter: 2390  \tTraining Loss: -501.4154    \n",
            "    Negative Log Likelihood: 31.8702\tSigma2 Prior: -533.2875\tRegularization: 0.0019\n",
            "Iter: 2400  \tTraining Loss: -515.1378    \n",
            "    Negative Log Likelihood: 29.9574\tSigma2 Prior: -545.0972\tRegularization: 0.0019\n",
            "Iter: 2410  \tTraining Loss: -473.7520    \n",
            "    Negative Log Likelihood: 37.3344\tSigma2 Prior: -511.0884\tRegularization: 0.0019\n",
            "Iter: 2420  \tTraining Loss: -476.7467    \n",
            "    Negative Log Likelihood: 29.5031\tSigma2 Prior: -506.2518\tRegularization: 0.0019\n",
            "Iter: 2430  \tTraining Loss: -498.6889    \n",
            "    Negative Log Likelihood: 29.7993\tSigma2 Prior: -528.4901\tRegularization: 0.0019\n",
            "Iter: 2440  \tTraining Loss: -485.4864    \n",
            "    Negative Log Likelihood: 34.0073\tSigma2 Prior: -519.4956\tRegularization: 0.0019\n",
            "Iter: 2450  \tTraining Loss: -523.6277    \n",
            "    Negative Log Likelihood: 31.6694\tSigma2 Prior: -555.2991\tRegularization: 0.0020\n",
            "Iter: 2460  \tTraining Loss: -499.7740    \n",
            "    Negative Log Likelihood: 40.6682\tSigma2 Prior: -540.4442\tRegularization: 0.0020\n",
            "Iter: 2470  \tTraining Loss: -508.0644    \n",
            "    Negative Log Likelihood: 35.3814\tSigma2 Prior: -543.4478\tRegularization: 0.0020\n",
            "Iter: 2480  \tTraining Loss: -492.7325    \n",
            "    Negative Log Likelihood: 34.1598\tSigma2 Prior: -526.8943\tRegularization: 0.0020\n",
            "Iter: 2490  \tTraining Loss: -467.1655    \n",
            "    Negative Log Likelihood: 40.2091\tSigma2 Prior: -507.3765\tRegularization: 0.0020\n",
            "Iter: 2500  \tTraining Loss: -504.0764    \n",
            "    Negative Log Likelihood: 35.2381\tSigma2 Prior: -539.3164\tRegularization: 0.0020\n",
            "Iter: 2510  \tTraining Loss: -448.9290    \n",
            "    Negative Log Likelihood: 36.9649\tSigma2 Prior: -485.8959\tRegularization: 0.0020\n",
            "Iter: 2520  \tTraining Loss: -529.4721    \n",
            "    Negative Log Likelihood: 31.5699\tSigma2 Prior: -561.0440\tRegularization: 0.0020\n",
            "Iter: 2530  \tTraining Loss: -484.7276    \n",
            "    Negative Log Likelihood: 43.2595\tSigma2 Prior: -527.9891\tRegularization: 0.0020\n",
            "Iter: 2540  \tTraining Loss: -463.3665    \n",
            "    Negative Log Likelihood: 31.4834\tSigma2 Prior: -494.8519\tRegularization: 0.0020\n",
            "Iter: 2550  \tTraining Loss: -484.8799    \n",
            "    Negative Log Likelihood: 33.7682\tSigma2 Prior: -518.6501\tRegularization: 0.0020\n",
            "Iter: 2560  \tTraining Loss: -470.3616    \n",
            "    Negative Log Likelihood: 38.5470\tSigma2 Prior: -508.9106\tRegularization: 0.0020\n",
            "Iter: 2570  \tTraining Loss: -499.7451    \n",
            "    Negative Log Likelihood: 33.5693\tSigma2 Prior: -533.3164\tRegularization: 0.0020\n",
            "Iter: 2580  \tTraining Loss: -483.4130    \n",
            "    Negative Log Likelihood: 37.5716\tSigma2 Prior: -520.9866\tRegularization: 0.0020\n",
            "Iter: 2590  \tTraining Loss: -442.4156    \n",
            "    Negative Log Likelihood: 38.8962\tSigma2 Prior: -481.3137\tRegularization: 0.0020\n",
            "Iter: 2600  \tTraining Loss: -489.8033    \n",
            "    Negative Log Likelihood: 33.1915\tSigma2 Prior: -522.9968\tRegularization: 0.0020\n",
            "Iter: 2610  \tTraining Loss: -513.0092    \n",
            "    Negative Log Likelihood: 39.9507\tSigma2 Prior: -552.9619\tRegularization: 0.0020\n",
            "Iter: 2620  \tTraining Loss: -504.1518    \n",
            "    Negative Log Likelihood: 31.6744\tSigma2 Prior: -535.8281\tRegularization: 0.0020\n",
            "Iter: 2630  \tTraining Loss: -504.1965    \n",
            "    Negative Log Likelihood: 38.2206\tSigma2 Prior: -542.4192\tRegularization: 0.0020\n",
            "Iter: 2640  \tTraining Loss: -487.3189    \n",
            "    Negative Log Likelihood: 31.0633\tSigma2 Prior: -518.3842\tRegularization: 0.0020\n",
            "Iter: 2650  \tTraining Loss: -469.5897    \n",
            "    Negative Log Likelihood: 31.2978\tSigma2 Prior: -500.8895\tRegularization: 0.0020\n",
            "Iter: 2660  \tTraining Loss: -484.9784    \n",
            "    Negative Log Likelihood: 33.0645\tSigma2 Prior: -518.0449\tRegularization: 0.0020\n",
            "Iter: 2670  \tTraining Loss: -493.4179    \n",
            "    Negative Log Likelihood: 32.2617\tSigma2 Prior: -525.6816\tRegularization: 0.0020\n",
            "Iter: 2680  \tTraining Loss: -509.4456    \n",
            "    Negative Log Likelihood: 33.3664\tSigma2 Prior: -542.8140\tRegularization: 0.0020\n",
            "Iter: 2690  \tTraining Loss: -504.1608    \n",
            "    Negative Log Likelihood: 29.0791\tSigma2 Prior: -533.2419\tRegularization: 0.0020\n",
            "Iter: 2700  \tTraining Loss: -488.1323    \n",
            "    Negative Log Likelihood: 40.1651\tSigma2 Prior: -528.2994\tRegularization: 0.0020\n",
            "Iter: 2710  \tTraining Loss: -504.4660    \n",
            "    Negative Log Likelihood: 25.6970\tSigma2 Prior: -530.1650\tRegularization: 0.0020\n",
            "Iter: 2720  \tTraining Loss: -527.6736    \n",
            "    Negative Log Likelihood: 36.5114\tSigma2 Prior: -564.1871\tRegularization: 0.0020\n",
            "Iter: 2730  \tTraining Loss: -512.6724    \n",
            "    Negative Log Likelihood: 38.2569\tSigma2 Prior: -550.9313\tRegularization: 0.0020\n",
            "Iter: 2740  \tTraining Loss: -478.9444    \n",
            "    Negative Log Likelihood: 33.4845\tSigma2 Prior: -512.4309\tRegularization: 0.0020\n",
            "Iter: 2750  \tTraining Loss: -459.8116    \n",
            "    Negative Log Likelihood: 30.9672\tSigma2 Prior: -490.7809\tRegularization: 0.0020\n",
            "Iter: 2760  \tTraining Loss: -509.0446    \n",
            "    Negative Log Likelihood: 33.5886\tSigma2 Prior: -542.6353\tRegularization: 0.0020\n",
            "Iter: 2770  \tTraining Loss: -510.4362    \n",
            "    Negative Log Likelihood: 33.1830\tSigma2 Prior: -543.6212\tRegularization: 0.0020\n",
            "Iter: 2780  \tTraining Loss: -476.3516    \n",
            "    Negative Log Likelihood: 33.9932\tSigma2 Prior: -510.3468\tRegularization: 0.0021\n",
            "Iter: 2790  \tTraining Loss: -464.5229    \n",
            "    Negative Log Likelihood: 36.7621\tSigma2 Prior: -501.2870\tRegularization: 0.0021\n",
            "Iter: 2800  \tTraining Loss: -487.6214    \n",
            "    Negative Log Likelihood: 30.5976\tSigma2 Prior: -518.2210\tRegularization: 0.0021\n",
            "Iter: 2810  \tTraining Loss: -484.1798    \n",
            "    Negative Log Likelihood: 39.2833\tSigma2 Prior: -523.4652\tRegularization: 0.0021\n",
            "Iter: 2820  \tTraining Loss: -468.9401    \n",
            "    Negative Log Likelihood: 28.4645\tSigma2 Prior: -497.4067\tRegularization: 0.0021\n",
            "Iter: 2830  \tTraining Loss: -477.6404    \n",
            "    Negative Log Likelihood: 29.9508\tSigma2 Prior: -507.5933\tRegularization: 0.0021\n",
            "Iter: 2840  \tTraining Loss: -461.9457    \n",
            "    Negative Log Likelihood: 40.6690\tSigma2 Prior: -502.6168\tRegularization: 0.0021\n",
            "Iter: 2850  \tTraining Loss: -506.8748    \n",
            "    Negative Log Likelihood: 31.8366\tSigma2 Prior: -538.7135\tRegularization: 0.0021\n",
            "Iter: 2860  \tTraining Loss: -501.4781    \n",
            "    Negative Log Likelihood: 31.2079\tSigma2 Prior: -532.6881\tRegularization: 0.0021\n",
            "Iter: 2870  \tTraining Loss: -501.8235    \n",
            "    Negative Log Likelihood: 31.2467\tSigma2 Prior: -533.0723\tRegularization: 0.0021\n",
            "Iter: 2880  \tTraining Loss: -445.3466    \n",
            "    Negative Log Likelihood: 40.7387\tSigma2 Prior: -486.0874\tRegularization: 0.0021\n",
            "Iter: 2890  \tTraining Loss: -476.6497    \n",
            "    Negative Log Likelihood: 32.1333\tSigma2 Prior: -508.7851\tRegularization: 0.0021\n",
            "Iter: 2900  \tTraining Loss: -504.4827    \n",
            "    Negative Log Likelihood: 36.1138\tSigma2 Prior: -540.5986\tRegularization: 0.0021\n",
            "Iter: 2910  \tTraining Loss: -517.0233    \n",
            "    Negative Log Likelihood: 33.4826\tSigma2 Prior: -550.5079\tRegularization: 0.0021\n",
            "Iter: 2920  \tTraining Loss: -504.0478    \n",
            "    Negative Log Likelihood: 34.0752\tSigma2 Prior: -538.1251\tRegularization: 0.0021\n",
            "Iter: 2930  \tTraining Loss: -485.6457    \n",
            "    Negative Log Likelihood: 31.5504\tSigma2 Prior: -517.1982\tRegularization: 0.0021\n",
            "Iter: 2940  \tTraining Loss: -504.7643    \n",
            "    Negative Log Likelihood: 34.0551\tSigma2 Prior: -538.8215\tRegularization: 0.0021\n",
            "Iter: 2950  \tTraining Loss: -432.4099    \n",
            "    Negative Log Likelihood: 32.7839\tSigma2 Prior: -465.1959\tRegularization: 0.0021\n",
            "Iter: 2960  \tTraining Loss: -484.1262    \n",
            "    Negative Log Likelihood: 35.2215\tSigma2 Prior: -519.3498\tRegularization: 0.0021\n",
            "Iter: 2970  \tTraining Loss: -475.0349    \n",
            "    Negative Log Likelihood: 36.4416\tSigma2 Prior: -511.4786\tRegularization: 0.0021\n",
            "Iter: 2980  \tTraining Loss: -500.4243    \n",
            "    Negative Log Likelihood: 28.5362\tSigma2 Prior: -528.9626\tRegularization: 0.0021\n",
            "Iter: 2990  \tTraining Loss: -432.2888    \n",
            "    Negative Log Likelihood: 37.0589\tSigma2 Prior: -469.3498\tRegularization: 0.0021\n",
            "Iter: 3000  \tTraining Loss: -473.6880    \n",
            "    Negative Log Likelihood: 26.9666\tSigma2 Prior: -500.6568\tRegularization: 0.0021\n",
            "Iter: 3010  \tTraining Loss: -509.9282    \n",
            "    Negative Log Likelihood: 32.9405\tSigma2 Prior: -542.8708\tRegularization: 0.0021\n",
            "Iter: 3020  \tTraining Loss: -520.2932    \n",
            "    Negative Log Likelihood: 35.7552\tSigma2 Prior: -556.0505\tRegularization: 0.0021\n",
            "Iter: 3030  \tTraining Loss: -474.8257    \n",
            "    Negative Log Likelihood: 30.4528\tSigma2 Prior: -505.2806\tRegularization: 0.0021\n",
            "Iter: 3040  \tTraining Loss: -510.2047    \n",
            "    Negative Log Likelihood: 33.6348\tSigma2 Prior: -543.8416\tRegularization: 0.0021\n",
            "Iter: 3050  \tTraining Loss: -493.7085    \n",
            "    Negative Log Likelihood: 33.8336\tSigma2 Prior: -527.5443\tRegularization: 0.0021\n",
            "Iter: 3060  \tTraining Loss: -487.3175    \n",
            "    Negative Log Likelihood: 32.0679\tSigma2 Prior: -519.3875\tRegularization: 0.0021\n",
            "Iter: 3070  \tTraining Loss: -517.7387    \n",
            "    Negative Log Likelihood: 31.3909\tSigma2 Prior: -549.1317\tRegularization: 0.0021\n",
            "Iter: 3080  \tTraining Loss: -518.0659    \n",
            "    Negative Log Likelihood: 34.4058\tSigma2 Prior: -552.4739\tRegularization: 0.0021\n",
            "Iter: 3090  \tTraining Loss: -482.7623    \n",
            "    Negative Log Likelihood: 31.1530\tSigma2 Prior: -513.9175\tRegularization: 0.0021\n",
            "Iter: 3100  \tTraining Loss: -494.7659    \n",
            "    Negative Log Likelihood: 38.0545\tSigma2 Prior: -532.8225\tRegularization: 0.0022\n",
            "Iter: 3110  \tTraining Loss: -474.8865    \n",
            "    Negative Log Likelihood: 29.7692\tSigma2 Prior: -504.6578\tRegularization: 0.0022\n",
            "Iter: 3120  \tTraining Loss: -518.6874    \n",
            "    Negative Log Likelihood: 31.3963\tSigma2 Prior: -550.0858\tRegularization: 0.0022\n",
            "Iter: 3130  \tTraining Loss: -509.3061    \n",
            "    Negative Log Likelihood: 42.3760\tSigma2 Prior: -551.6843\tRegularization: 0.0022\n",
            "Iter: 3140  \tTraining Loss: -500.4389    \n",
            "    Negative Log Likelihood: 37.0581\tSigma2 Prior: -537.4991\tRegularization: 0.0022\n",
            "Iter: 3150  \tTraining Loss: -466.5859    \n",
            "    Negative Log Likelihood: 35.7013\tSigma2 Prior: -502.2893\tRegularization: 0.0022\n",
            "Iter: 3160  \tTraining Loss: -469.1182    \n",
            "    Negative Log Likelihood: 33.6584\tSigma2 Prior: -502.7787\tRegularization: 0.0022\n",
            "Iter: 3170  \tTraining Loss: -493.1935    \n",
            "    Negative Log Likelihood: 37.6759\tSigma2 Prior: -530.8716\tRegularization: 0.0022\n",
            "Iter: 3180  \tTraining Loss: -521.4244    \n",
            "    Negative Log Likelihood: 30.4316\tSigma2 Prior: -551.8582\tRegularization: 0.0022\n",
            "Iter: 3190  \tTraining Loss: -505.5438    \n",
            "    Negative Log Likelihood: 34.0918\tSigma2 Prior: -539.6378\tRegularization: 0.0022\n",
            "Iter: 3200  \tTraining Loss: -500.4985    \n",
            "    Negative Log Likelihood: 35.6443\tSigma2 Prior: -536.1450\tRegularization: 0.0022\n",
            "Iter: 3210  \tTraining Loss: -486.4790    \n",
            "    Negative Log Likelihood: 35.5128\tSigma2 Prior: -521.9940\tRegularization: 0.0022\n",
            "Iter: 3220  \tTraining Loss: -511.8386    \n",
            "    Negative Log Likelihood: 35.6968\tSigma2 Prior: -547.5376\tRegularization: 0.0022\n",
            "Iter: 3230  \tTraining Loss: -486.2185    \n",
            "    Negative Log Likelihood: 31.6383\tSigma2 Prior: -517.8590\tRegularization: 0.0022\n",
            "Iter: 3240  \tTraining Loss: -500.2784    \n",
            "    Negative Log Likelihood: 34.8448\tSigma2 Prior: -535.1254\tRegularization: 0.0022\n",
            "Iter: 3250  \tTraining Loss: -516.7042    \n",
            "    Negative Log Likelihood: 37.7791\tSigma2 Prior: -554.4855\tRegularization: 0.0022\n",
            "Iter: 3260  \tTraining Loss: -505.4648    \n",
            "    Negative Log Likelihood: 32.3561\tSigma2 Prior: -537.8231\tRegularization: 0.0022\n",
            "Iter: 3270  \tTraining Loss: -505.9079    \n",
            "    Negative Log Likelihood: 37.2154\tSigma2 Prior: -543.1255\tRegularization: 0.0022\n",
            "Iter: 3280  \tTraining Loss: -518.3937    \n",
            "    Negative Log Likelihood: 30.2563\tSigma2 Prior: -548.6523\tRegularization: 0.0022\n",
            "Iter: 3290  \tTraining Loss: -479.6116    \n",
            "    Negative Log Likelihood: 38.8946\tSigma2 Prior: -518.5084\tRegularization: 0.0022\n",
            "Iter: 3300  \tTraining Loss: -532.8851    \n",
            "    Negative Log Likelihood: 34.1502\tSigma2 Prior: -567.0375\tRegularization: 0.0022\n",
            "Iter: 3310  \tTraining Loss: -508.6331    \n",
            "    Negative Log Likelihood: 34.9305\tSigma2 Prior: -543.5658\tRegularization: 0.0022\n",
            "Iter: 3320  \tTraining Loss: -486.4404    \n",
            "    Negative Log Likelihood: 33.1953\tSigma2 Prior: -519.6379\tRegularization: 0.0022\n",
            "Iter: 3330  \tTraining Loss: -501.8754    \n",
            "    Negative Log Likelihood: 33.2339\tSigma2 Prior: -535.1115\tRegularization: 0.0022\n",
            "Iter: 3340  \tTraining Loss: -483.7582    \n",
            "    Negative Log Likelihood: 35.7982\tSigma2 Prior: -519.5586\tRegularization: 0.0022\n",
            "Iter: 3350  \tTraining Loss: -481.2902    \n",
            "    Negative Log Likelihood: 33.3465\tSigma2 Prior: -514.6389\tRegularization: 0.0022\n",
            "Iter: 3360  \tTraining Loss: -514.4131    \n",
            "    Negative Log Likelihood: 37.3090\tSigma2 Prior: -551.7244\tRegularization: 0.0022\n",
            "Iter: 3370  \tTraining Loss: -483.4990    \n",
            "    Negative Log Likelihood: 32.7762\tSigma2 Prior: -516.2775\tRegularization: 0.0022\n",
            "Iter: 3380  \tTraining Loss: -505.5256    \n",
            "    Negative Log Likelihood: 34.8320\tSigma2 Prior: -540.3598\tRegularization: 0.0022\n",
            "Iter: 3390  \tTraining Loss: -467.1068    \n",
            "    Negative Log Likelihood: 30.6448\tSigma2 Prior: -497.7537\tRegularization: 0.0022\n",
            "Iter: 3400  \tTraining Loss: -490.0571    \n",
            "    Negative Log Likelihood: 32.3011\tSigma2 Prior: -522.3604\tRegularization: 0.0022\n",
            "Iter: 3410  \tTraining Loss: -512.9610    \n",
            "    Negative Log Likelihood: 33.3102\tSigma2 Prior: -546.2735\tRegularization: 0.0022\n",
            "Iter: 3420  \tTraining Loss: -511.6458    \n",
            "    Negative Log Likelihood: 29.7455\tSigma2 Prior: -541.3936\tRegularization: 0.0022\n",
            "Iter: 3430  \tTraining Loss: -499.4690    \n",
            "    Negative Log Likelihood: 36.1957\tSigma2 Prior: -535.6669\tRegularization: 0.0023\n",
            "Iter: 3440  \tTraining Loss: -465.8668    \n",
            "    Negative Log Likelihood: 35.7684\tSigma2 Prior: -501.6375\tRegularization: 0.0023\n",
            "Iter: 3450  \tTraining Loss: -517.9880    \n",
            "    Negative Log Likelihood: 35.5342\tSigma2 Prior: -553.5245\tRegularization: 0.0023\n",
            "Iter: 3460  \tTraining Loss: -494.5077    \n",
            "    Negative Log Likelihood: 28.1084\tSigma2 Prior: -522.6183\tRegularization: 0.0023\n",
            "Iter: 3470  \tTraining Loss: -495.8727    \n",
            "    Negative Log Likelihood: 36.8075\tSigma2 Prior: -532.6824\tRegularization: 0.0023\n",
            "Iter: 3480  \tTraining Loss: -498.8903    \n",
            "    Negative Log Likelihood: 31.0030\tSigma2 Prior: -529.8955\tRegularization: 0.0023\n",
            "Iter: 3490  \tTraining Loss: -490.6413    \n",
            "    Negative Log Likelihood: 31.0237\tSigma2 Prior: -521.6672\tRegularization: 0.0023\n",
            "Iter: 3500  \tTraining Loss: -499.2862    \n",
            "    Negative Log Likelihood: 33.7442\tSigma2 Prior: -533.0326\tRegularization: 0.0023\n",
            "Iter: 3510  \tTraining Loss: -515.4852    \n",
            "    Negative Log Likelihood: 35.9254\tSigma2 Prior: -551.4129\tRegularization: 0.0023\n",
            "Iter: 3520  \tTraining Loss: -497.5370    \n",
            "    Negative Log Likelihood: 27.6824\tSigma2 Prior: -525.2217\tRegularization: 0.0023\n",
            "Iter: 3530  \tTraining Loss: -465.6539    \n",
            "    Negative Log Likelihood: 35.4166\tSigma2 Prior: -501.0728\tRegularization: 0.0023\n",
            "Iter: 3540  \tTraining Loss: -506.1835    \n",
            "    Negative Log Likelihood: 38.8774\tSigma2 Prior: -545.0632\tRegularization: 0.0023\n",
            "Iter: 3550  \tTraining Loss: -432.0516    \n",
            "    Negative Log Likelihood: 33.1621\tSigma2 Prior: -465.2160\tRegularization: 0.0023\n",
            "Iter: 3560  \tTraining Loss: -498.0305    \n",
            "    Negative Log Likelihood: 39.2216\tSigma2 Prior: -537.2545\tRegularization: 0.0023\n",
            "Iter: 3570  \tTraining Loss: -473.0530    \n",
            "    Negative Log Likelihood: 31.6909\tSigma2 Prior: -504.7462\tRegularization: 0.0023\n",
            "Iter: 3580  \tTraining Loss: -492.9915    \n",
            "    Negative Log Likelihood: 33.0527\tSigma2 Prior: -526.0464\tRegularization: 0.0023\n",
            "Iter: 3590  \tTraining Loss: -517.5781    \n",
            "    Negative Log Likelihood: 35.5522\tSigma2 Prior: -553.1327\tRegularization: 0.0023\n",
            "Iter: 3600  \tTraining Loss: -422.6541    \n",
            "    Negative Log Likelihood: 32.6263\tSigma2 Prior: -455.2828\tRegularization: 0.0023\n",
            "Iter: 3610  \tTraining Loss: -518.1093    \n",
            "    Negative Log Likelihood: 30.7774\tSigma2 Prior: -548.8890\tRegularization: 0.0023\n",
            "Iter: 3620  \tTraining Loss: -478.9183    \n",
            "    Negative Log Likelihood: 40.5230\tSigma2 Prior: -519.4436\tRegularization: 0.0023\n",
            "Iter: 3630  \tTraining Loss: -494.7667    \n",
            "    Negative Log Likelihood: 31.2387\tSigma2 Prior: -526.0078\tRegularization: 0.0023\n",
            "Iter: 3640  \tTraining Loss: -483.5510    \n",
            "    Negative Log Likelihood: 36.9017\tSigma2 Prior: -520.4550\tRegularization: 0.0023\n",
            "Iter: 3650  \tTraining Loss: -491.1925    \n",
            "    Negative Log Likelihood: 35.1471\tSigma2 Prior: -526.3419\tRegularization: 0.0023\n",
            "Iter: 3660  \tTraining Loss: -508.5753    \n",
            "    Negative Log Likelihood: 36.6069\tSigma2 Prior: -545.1845\tRegularization: 0.0023\n",
            "Iter: 3670  \tTraining Loss: -494.5082    \n",
            "    Negative Log Likelihood: 32.5234\tSigma2 Prior: -527.0339\tRegularization: 0.0023\n",
            "Iter: 3680  \tTraining Loss: -506.7620    \n",
            "    Negative Log Likelihood: 29.3436\tSigma2 Prior: -536.1079\tRegularization: 0.0023\n",
            "Iter: 3690  \tTraining Loss: -492.0838    \n",
            "    Negative Log Likelihood: 36.6445\tSigma2 Prior: -528.7306\tRegularization: 0.0023\n",
            "Iter: 3700  \tTraining Loss: -505.7175    \n",
            "    Negative Log Likelihood: 35.6817\tSigma2 Prior: -541.4015\tRegularization: 0.0023\n",
            "Iter: 3710  \tTraining Loss: -479.2960    \n",
            "    Negative Log Likelihood: 34.7505\tSigma2 Prior: -514.0488\tRegularization: 0.0023\n",
            "Iter: 3720  \tTraining Loss: -497.8172    \n",
            "    Negative Log Likelihood: 35.8971\tSigma2 Prior: -533.7167\tRegularization: 0.0023\n",
            "Iter: 3730  \tTraining Loss: -534.0165    \n",
            "    Negative Log Likelihood: 31.4118\tSigma2 Prior: -565.4307\tRegularization: 0.0023\n",
            "Iter: 3740  \tTraining Loss: -487.9890    \n",
            "    Negative Log Likelihood: 29.7895\tSigma2 Prior: -517.7809\tRegularization: 0.0023\n",
            "Iter: 3750  \tTraining Loss: -429.9746    \n",
            "    Negative Log Likelihood: 35.3929\tSigma2 Prior: -465.3699\tRegularization: 0.0024\n",
            "Iter: 3760  \tTraining Loss: -507.4868    \n",
            "    Negative Log Likelihood: 38.4890\tSigma2 Prior: -545.9781\tRegularization: 0.0024\n",
            "Iter: 3770  \tTraining Loss: -477.8712    \n",
            "    Negative Log Likelihood: 37.5058\tSigma2 Prior: -515.3793\tRegularization: 0.0024\n",
            "Iter: 3780  \tTraining Loss: -468.2312    \n",
            "    Negative Log Likelihood: 32.1459\tSigma2 Prior: -500.3795\tRegularization: 0.0024\n",
            "Iter: 3790  \tTraining Loss: -474.2595    \n",
            "    Negative Log Likelihood: 32.8984\tSigma2 Prior: -507.1603\tRegularization: 0.0024\n",
            "Iter: 3800  \tTraining Loss: -492.1521    \n",
            "    Negative Log Likelihood: 36.5429\tSigma2 Prior: -528.6974\tRegularization: 0.0024\n",
            "Iter: 3810  \tTraining Loss: -509.8715    \n",
            "    Negative Log Likelihood: 29.9135\tSigma2 Prior: -539.7874\tRegularization: 0.0024\n",
            "Iter: 3820  \tTraining Loss: -423.5083    \n",
            "    Negative Log Likelihood: 38.3390\tSigma2 Prior: -461.8497\tRegularization: 0.0024\n",
            "Iter: 3830  \tTraining Loss: -507.8057    \n",
            "    Negative Log Likelihood: 33.9549\tSigma2 Prior: -541.7630\tRegularization: 0.0024\n",
            "Iter: 3840  \tTraining Loss: -508.9290    \n",
            "    Negative Log Likelihood: 31.0100\tSigma2 Prior: -539.9413\tRegularization: 0.0024\n",
            "Iter: 3850  \tTraining Loss: -487.5667    \n",
            "    Negative Log Likelihood: 32.1513\tSigma2 Prior: -519.7205\tRegularization: 0.0024\n",
            "Iter: 3860  \tTraining Loss: -433.5486    \n",
            "    Negative Log Likelihood: 39.7716\tSigma2 Prior: -473.3226\tRegularization: 0.0024\n",
            "Iter: 3870  \tTraining Loss: -485.7114    \n",
            "    Negative Log Likelihood: 30.7541\tSigma2 Prior: -516.4679\tRegularization: 0.0024\n",
            "Iter: 3880  \tTraining Loss: -451.6405    \n",
            "    Negative Log Likelihood: 36.3763\tSigma2 Prior: -488.0192\tRegularization: 0.0024\n",
            "Iter: 3890  \tTraining Loss: -473.2865    \n",
            "    Negative Log Likelihood: 34.6075\tSigma2 Prior: -507.8964\tRegularization: 0.0024\n",
            "Iter: 3900  \tTraining Loss: -519.3832    \n",
            "    Negative Log Likelihood: 34.4736\tSigma2 Prior: -553.8593\tRegularization: 0.0024\n",
            "Iter: 3910  \tTraining Loss: -486.3943    \n",
            "    Negative Log Likelihood: 36.5712\tSigma2 Prior: -522.9679\tRegularization: 0.0024\n",
            "Iter: 3920  \tTraining Loss: -483.8340    \n",
            "    Negative Log Likelihood: 34.6423\tSigma2 Prior: -518.4787\tRegularization: 0.0024\n",
            "Iter: 3930  \tTraining Loss: -493.1855    \n",
            "    Negative Log Likelihood: 34.5640\tSigma2 Prior: -527.7518\tRegularization: 0.0024\n",
            "Iter: 3940  \tTraining Loss: -491.1537    \n",
            "    Negative Log Likelihood: 39.8265\tSigma2 Prior: -530.9826\tRegularization: 0.0024\n",
            "Iter: 3950  \tTraining Loss: -459.5668    \n",
            "    Negative Log Likelihood: 29.1630\tSigma2 Prior: -488.7322\tRegularization: 0.0024\n",
            "Iter: 3960  \tTraining Loss: -503.8419    \n",
            "    Negative Log Likelihood: 32.4253\tSigma2 Prior: -536.2697\tRegularization: 0.0024\n",
            "Iter: 3970  \tTraining Loss: -505.6335    \n",
            "    Negative Log Likelihood: 32.5844\tSigma2 Prior: -538.2203\tRegularization: 0.0024\n",
            "Iter: 3980  \tTraining Loss: -493.0405    \n",
            "    Negative Log Likelihood: 33.9312\tSigma2 Prior: -526.9742\tRegularization: 0.0024\n",
            "Iter: 3990  \tTraining Loss: -451.1331    \n",
            "    Negative Log Likelihood: 40.6369\tSigma2 Prior: -491.7724\tRegularization: 0.0024\n",
            "Iter: 4000  \tTraining Loss: -489.7391    \n",
            "    Negative Log Likelihood: 30.2570\tSigma2 Prior: -519.9986\tRegularization: 0.0024\n",
            "Iter: 4010  \tTraining Loss: -508.0554    \n",
            "    Negative Log Likelihood: 34.6140\tSigma2 Prior: -542.6718\tRegularization: 0.0024\n",
            "Iter: 4020  \tTraining Loss: -454.6501    \n",
            "    Negative Log Likelihood: 41.7017\tSigma2 Prior: -496.3542\tRegularization: 0.0024\n",
            "Iter: 4030  \tTraining Loss: -510.0104    \n",
            "    Negative Log Likelihood: 31.8111\tSigma2 Prior: -541.8240\tRegularization: 0.0024\n",
            "Iter: 4040  \tTraining Loss: -506.2875    \n",
            "    Negative Log Likelihood: 38.1514\tSigma2 Prior: -544.4413\tRegularization: 0.0024\n",
            "Iter: 4050  \tTraining Loss: -504.5274    \n",
            "    Negative Log Likelihood: 31.5552\tSigma2 Prior: -536.0850\tRegularization: 0.0024\n",
            "Iter: 4060  \tTraining Loss: -488.0749    \n",
            "    Negative Log Likelihood: 36.5479\tSigma2 Prior: -524.6253\tRegularization: 0.0025\n",
            "Iter: 4070  \tTraining Loss: -470.4957    \n",
            "    Negative Log Likelihood: 31.6232\tSigma2 Prior: -502.1214\tRegularization: 0.0025\n",
            "Iter: 4080  \tTraining Loss: -458.0766    \n",
            "    Negative Log Likelihood: 35.9093\tSigma2 Prior: -493.9885\tRegularization: 0.0025\n",
            "Iter: 4090  \tTraining Loss: -488.3773    \n",
            "    Negative Log Likelihood: 31.3539\tSigma2 Prior: -519.7338\tRegularization: 0.0025\n",
            "Iter: 4100  \tTraining Loss: -451.5182    \n",
            "    Negative Log Likelihood: 35.5726\tSigma2 Prior: -487.0932\tRegularization: 0.0025\n",
            "Iter: 4110  \tTraining Loss: -461.3948    \n",
            "    Negative Log Likelihood: 38.7814\tSigma2 Prior: -500.1787\tRegularization: 0.0025\n",
            "Iter: 4120  \tTraining Loss: -477.7005    \n",
            "    Negative Log Likelihood: 31.2530\tSigma2 Prior: -508.9560\tRegularization: 0.0025\n",
            "Iter: 4130  \tTraining Loss: -456.8714    \n",
            "    Negative Log Likelihood: 32.2442\tSigma2 Prior: -489.1180\tRegularization: 0.0025\n",
            "Iter: 4140  \tTraining Loss: -362.2306    \n",
            "    Negative Log Likelihood: 37.7910\tSigma2 Prior: -400.0241\tRegularization: 0.0025\n",
            "Iter: 4150  \tTraining Loss: -487.5957    \n",
            "    Negative Log Likelihood: 27.3803\tSigma2 Prior: -514.9785\tRegularization: 0.0025\n",
            "Iter: 4160  \tTraining Loss: -486.6667    \n",
            "    Negative Log Likelihood: 38.8673\tSigma2 Prior: -525.5364\tRegularization: 0.0025\n",
            "Iter: 4170  \tTraining Loss: -500.8664    \n",
            "    Negative Log Likelihood: 35.3096\tSigma2 Prior: -536.1785\tRegularization: 0.0025\n",
            "Iter: 4180  \tTraining Loss: -504.3326    \n",
            "    Negative Log Likelihood: 33.2130\tSigma2 Prior: -537.5481\tRegularization: 0.0025\n",
            "Iter: 4190  \tTraining Loss: -493.4571    \n",
            "    Negative Log Likelihood: 31.8683\tSigma2 Prior: -525.3279\tRegularization: 0.0025\n",
            "Iter: 4200  \tTraining Loss: -483.2463    \n",
            "    Negative Log Likelihood: 36.2311\tSigma2 Prior: -519.4799\tRegularization: 0.0025\n",
            "Iter: 4210  \tTraining Loss: -495.3596    \n",
            "    Negative Log Likelihood: 31.3027\tSigma2 Prior: -526.6648\tRegularization: 0.0025\n",
            "Iter: 4220  \tTraining Loss: -500.0071    \n",
            "    Negative Log Likelihood: 36.4159\tSigma2 Prior: -536.4255\tRegularization: 0.0025\n",
            "Iter: 4230  \tTraining Loss: -481.4321    \n",
            "    Negative Log Likelihood: 33.2624\tSigma2 Prior: -514.6970\tRegularization: 0.0025\n",
            "Iter: 4240  \tTraining Loss: -516.4072    \n",
            "    Negative Log Likelihood: 33.3459\tSigma2 Prior: -549.7556\tRegularization: 0.0025\n",
            "Iter: 4250  \tTraining Loss: -453.2808    \n",
            "    Negative Log Likelihood: 36.8197\tSigma2 Prior: -490.1030\tRegularization: 0.0025\n",
            "Iter: 4260  \tTraining Loss: -493.5382    \n",
            "    Negative Log Likelihood: 26.6542\tSigma2 Prior: -520.1949\tRegularization: 0.0025\n",
            "Iter: 4270  \tTraining Loss: -500.4702    \n",
            "    Negative Log Likelihood: 38.0402\tSigma2 Prior: -538.5129\tRegularization: 0.0025\n",
            "Iter: 4280  \tTraining Loss: -472.0765    \n",
            "    Negative Log Likelihood: 26.8904\tSigma2 Prior: -498.9694\tRegularization: 0.0025\n",
            "Iter: 4290  \tTraining Loss: -470.9722    \n",
            "    Negative Log Likelihood: 30.7115\tSigma2 Prior: -501.6862\tRegularization: 0.0025\n",
            "Iter: 4300  \tTraining Loss: -516.3668    \n",
            "    Negative Log Likelihood: 38.2104\tSigma2 Prior: -554.5797\tRegularization: 0.0025\n",
            "Iter: 4310  \tTraining Loss: -459.1963    \n",
            "    Negative Log Likelihood: 32.2041\tSigma2 Prior: -491.4029\tRegularization: 0.0025\n",
            "Iter: 4320  \tTraining Loss: -428.7531    \n",
            "    Negative Log Likelihood: 36.3429\tSigma2 Prior: -465.0986\tRegularization: 0.0025\n",
            "Iter: 4330  \tTraining Loss: -494.5585    \n",
            "    Negative Log Likelihood: 32.6549\tSigma2 Prior: -527.2160\tRegularization: 0.0025\n",
            "Iter: 4340  \tTraining Loss: -496.3245    \n",
            "    Negative Log Likelihood: 31.8933\tSigma2 Prior: -528.2203\tRegularization: 0.0025\n",
            "Iter: 4350  \tTraining Loss: -459.2561    \n",
            "    Negative Log Likelihood: 34.6894\tSigma2 Prior: -493.9481\tRegularization: 0.0025\n",
            "Iter: 4360  \tTraining Loss: -516.9026    \n",
            "    Negative Log Likelihood: 33.5695\tSigma2 Prior: -550.4747\tRegularization: 0.0025\n",
            "Iter: 4370  \tTraining Loss: -510.7268    \n",
            "    Negative Log Likelihood: 36.8208\tSigma2 Prior: -547.5502\tRegularization: 0.0026\n",
            "Iter: 4380  \tTraining Loss: -432.2317    \n",
            "    Negative Log Likelihood: 34.2717\tSigma2 Prior: -466.5059\tRegularization: 0.0026\n",
            "Iter: 4390  \tTraining Loss: -499.4336    \n",
            "    Negative Log Likelihood: 33.6009\tSigma2 Prior: -533.0370\tRegularization: 0.0026\n",
            "Iter: 4400  \tTraining Loss: -507.1007    \n",
            "    Negative Log Likelihood: 38.0089\tSigma2 Prior: -545.1122\tRegularization: 0.0026\n",
            "Iter: 4410  \tTraining Loss: -474.3364    \n",
            "    Negative Log Likelihood: 32.7940\tSigma2 Prior: -507.1329\tRegularization: 0.0026\n",
            "Iter: 4420  \tTraining Loss: -497.5257    \n",
            "    Negative Log Likelihood: 31.1439\tSigma2 Prior: -528.6722\tRegularization: 0.0026\n",
            "Iter: 4430  \tTraining Loss: -478.4119    \n",
            "    Negative Log Likelihood: 35.2541\tSigma2 Prior: -513.6685\tRegularization: 0.0026\n",
            "Iter: 4440  \tTraining Loss: -484.8220    \n",
            "    Negative Log Likelihood: 34.6280\tSigma2 Prior: -519.4526\tRegularization: 0.0026\n",
            "Iter: 4450  \tTraining Loss: -469.6520    \n",
            "    Negative Log Likelihood: 32.8648\tSigma2 Prior: -502.5193\tRegularization: 0.0026\n",
            "Iter: 4460  \tTraining Loss: -467.5135    \n",
            "    Negative Log Likelihood: 33.2775\tSigma2 Prior: -500.7937\tRegularization: 0.0026\n",
            "Iter: 4470  \tTraining Loss: -485.6114    \n",
            "    Negative Log Likelihood: 37.3151\tSigma2 Prior: -522.9291\tRegularization: 0.0026\n",
            "Iter: 4480  \tTraining Loss: -473.2271    \n",
            "    Negative Log Likelihood: 35.3686\tSigma2 Prior: -508.5984\tRegularization: 0.0026\n",
            "Iter: 4490  \tTraining Loss: -516.0605    \n",
            "    Negative Log Likelihood: 33.8775\tSigma2 Prior: -549.9406\tRegularization: 0.0026\n",
            "Iter: 4500  \tTraining Loss: -506.1570    \n",
            "    Negative Log Likelihood: 31.9289\tSigma2 Prior: -538.0884\tRegularization: 0.0026\n",
            "Iter: 4510  \tTraining Loss: -518.3585    \n",
            "    Negative Log Likelihood: 34.2457\tSigma2 Prior: -552.6068\tRegularization: 0.0026\n",
            "Iter: 4520  \tTraining Loss: -499.8033    \n",
            "    Negative Log Likelihood: 30.9233\tSigma2 Prior: -530.7291\tRegularization: 0.0026\n",
            "Iter: 4530  \tTraining Loss: -514.7892    \n",
            "    Negative Log Likelihood: 37.3572\tSigma2 Prior: -552.1490\tRegularization: 0.0026\n",
            "Iter: 4540  \tTraining Loss: -517.9720    \n",
            "    Negative Log Likelihood: 34.5633\tSigma2 Prior: -552.5380\tRegularization: 0.0026\n",
            "Iter: 4550  \tTraining Loss: -523.6279    \n",
            "    Negative Log Likelihood: 36.5923\tSigma2 Prior: -560.2229\tRegularization: 0.0026\n",
            "Iter: 4560  \tTraining Loss: -474.3455    \n",
            "    Negative Log Likelihood: 32.6430\tSigma2 Prior: -506.9911\tRegularization: 0.0026\n",
            "Iter: 4570  \tTraining Loss: -472.1238    \n",
            "    Negative Log Likelihood: 36.6851\tSigma2 Prior: -508.8116\tRegularization: 0.0026\n",
            "Iter: 4580  \tTraining Loss: -502.7906    \n",
            "    Negative Log Likelihood: 35.8931\tSigma2 Prior: -538.6864\tRegularization: 0.0026\n",
            "Iter: 4590  \tTraining Loss: -434.8442    \n",
            "    Negative Log Likelihood: 33.2423\tSigma2 Prior: -468.0891\tRegularization: 0.0026\n",
            "Iter: 4600  \tTraining Loss: -447.5701    \n",
            "    Negative Log Likelihood: 38.4506\tSigma2 Prior: -486.0233\tRegularization: 0.0026\n",
            "Iter: 4610  \tTraining Loss: -472.5788    \n",
            "    Negative Log Likelihood: 31.2055\tSigma2 Prior: -503.7869\tRegularization: 0.0026\n",
            "Iter: 4620  \tTraining Loss: -508.6232    \n",
            "    Negative Log Likelihood: 28.9805\tSigma2 Prior: -537.6063\tRegularization: 0.0026\n",
            "Iter: 4630  \tTraining Loss: -505.1036    \n",
            "    Negative Log Likelihood: 41.0841\tSigma2 Prior: -546.1903\tRegularization: 0.0026\n",
            "Iter: 4640  \tTraining Loss: -509.3990    \n",
            "    Negative Log Likelihood: 31.0365\tSigma2 Prior: -540.4381\tRegularization: 0.0026\n",
            "Iter: 4650  \tTraining Loss: -489.9900    \n",
            "    Negative Log Likelihood: 33.6158\tSigma2 Prior: -523.6085\tRegularization: 0.0026\n",
            "Iter: 4660  \tTraining Loss: -501.4288    \n",
            "    Negative Log Likelihood: 36.9101\tSigma2 Prior: -538.3416\tRegularization: 0.0026\n",
            "Iter: 4670  \tTraining Loss: -495.5120    \n",
            "    Negative Log Likelihood: 26.6975\tSigma2 Prior: -522.2122\tRegularization: 0.0026\n",
            "Iter: 4680  \tTraining Loss: -514.8520    \n",
            "    Negative Log Likelihood: 31.2831\tSigma2 Prior: -546.1377\tRegularization: 0.0027\n",
            "Iter: 4690  \tTraining Loss: -461.3535    \n",
            "    Negative Log Likelihood: 34.1228\tSigma2 Prior: -495.4789\tRegularization: 0.0027\n",
            "Iter: 4700  \tTraining Loss: -510.1570    \n",
            "    Negative Log Likelihood: 32.1297\tSigma2 Prior: -542.2892\tRegularization: 0.0027\n",
            "Iter: 4710  \tTraining Loss: -523.4754    \n",
            "    Negative Log Likelihood: 36.0740\tSigma2 Prior: -559.5521\tRegularization: 0.0027\n",
            "Iter: 4720  \tTraining Loss: -474.3330    \n",
            "    Negative Log Likelihood: 33.9180\tSigma2 Prior: -508.2537\tRegularization: 0.0027\n",
            "Iter: 4730  \tTraining Loss: -496.8451    \n",
            "    Negative Log Likelihood: 32.2224\tSigma2 Prior: -529.0701\tRegularization: 0.0027\n",
            "Iter: 4740  \tTraining Loss: -490.8257    \n",
            "    Negative Log Likelihood: 29.8547\tSigma2 Prior: -520.6830\tRegularization: 0.0027\n",
            "Iter: 4750  \tTraining Loss: -497.8373    \n",
            "    Negative Log Likelihood: 33.7896\tSigma2 Prior: -531.6296\tRegularization: 0.0027\n",
            "Iter: 4760  \tTraining Loss: -500.6804    \n",
            "    Negative Log Likelihood: 32.1428\tSigma2 Prior: -532.8259\tRegularization: 0.0027\n",
            "Iter: 4770  \tTraining Loss: -492.8749    \n",
            "    Negative Log Likelihood: 28.6612\tSigma2 Prior: -521.5388\tRegularization: 0.0027\n",
            "Iter: 4780  \tTraining Loss: -510.1252    \n",
            "    Negative Log Likelihood: 30.6441\tSigma2 Prior: -540.7720\tRegularization: 0.0027\n",
            "Iter: 4790  \tTraining Loss: -473.5681    \n",
            "    Negative Log Likelihood: 36.7395\tSigma2 Prior: -510.3104\tRegularization: 0.0027\n",
            "Iter: 4800  \tTraining Loss: -503.8263    \n",
            "    Negative Log Likelihood: 30.9157\tSigma2 Prior: -534.7446\tRegularization: 0.0027\n",
            "Iter: 4810  \tTraining Loss: -467.0606    \n",
            "    Negative Log Likelihood: 33.0716\tSigma2 Prior: -500.1349\tRegularization: 0.0027\n",
            "Iter: 4820  \tTraining Loss: -496.5621    \n",
            "    Negative Log Likelihood: 34.0350\tSigma2 Prior: -530.5997\tRegularization: 0.0027\n",
            "Iter: 4830  \tTraining Loss: -497.7303    \n",
            "    Negative Log Likelihood: 35.5948\tSigma2 Prior: -533.3278\tRegularization: 0.0027\n",
            "Iter: 4840  \tTraining Loss: -490.4529    \n",
            "    Negative Log Likelihood: 33.3851\tSigma2 Prior: -523.8407\tRegularization: 0.0027\n",
            "Iter: 4850  \tTraining Loss: -496.9584    \n",
            "    Negative Log Likelihood: 30.4677\tSigma2 Prior: -527.4288\tRegularization: 0.0027\n",
            "Iter: 4860  \tTraining Loss: -490.9428    \n",
            "    Negative Log Likelihood: 34.6594\tSigma2 Prior: -525.6050\tRegularization: 0.0027\n",
            "Iter: 4870  \tTraining Loss: -508.6754    \n",
            "    Negative Log Likelihood: 36.9640\tSigma2 Prior: -545.6422\tRegularization: 0.0027\n",
            "Iter: 4880  \tTraining Loss: -499.4180    \n",
            "    Negative Log Likelihood: 30.1882\tSigma2 Prior: -529.6089\tRegularization: 0.0027\n",
            "Iter: 4890  \tTraining Loss: -459.7657    \n",
            "    Negative Log Likelihood: 35.7509\tSigma2 Prior: -495.5193\tRegularization: 0.0027\n",
            "Iter: 4900  \tTraining Loss: -482.4715    \n",
            "    Negative Log Likelihood: 31.7154\tSigma2 Prior: -514.1896\tRegularization: 0.0027\n",
            "Iter: 4910  \tTraining Loss: -443.4632    \n",
            "    Negative Log Likelihood: 36.5183\tSigma2 Prior: -479.9843\tRegularization: 0.0027\n",
            "Iter: 4920  \tTraining Loss: -473.6556    \n",
            "    Negative Log Likelihood: 28.5722\tSigma2 Prior: -502.2305\tRegularization: 0.0027\n",
            "Iter: 4930  \tTraining Loss: -505.9994    \n",
            "    Negative Log Likelihood: 36.9140\tSigma2 Prior: -542.9161\tRegularization: 0.0027\n",
            "Iter: 4940  \tTraining Loss: -515.0165    \n",
            "    Negative Log Likelihood: 31.9400\tSigma2 Prior: -546.9593\tRegularization: 0.0027\n",
            "Iter: 4950  \tTraining Loss: -487.4056    \n",
            "    Negative Log Likelihood: 36.9776\tSigma2 Prior: -524.3860\tRegularization: 0.0027\n",
            "Iter: 4960  \tTraining Loss: -483.0831    \n",
            "    Negative Log Likelihood: 32.8983\tSigma2 Prior: -515.9841\tRegularization: 0.0027\n",
            "Iter: 4970  \tTraining Loss: -463.7314    \n",
            "    Negative Log Likelihood: 33.2092\tSigma2 Prior: -496.9433\tRegularization: 0.0027\n",
            "Iter: 4980  \tTraining Loss: -506.6988    \n",
            "    Negative Log Likelihood: 35.1186\tSigma2 Prior: -541.8201\tRegularization: 0.0027\n",
            "Iter: 4990  \tTraining Loss: -460.9178    \n",
            "    Negative Log Likelihood: 32.0887\tSigma2 Prior: -493.0093\tRegularization: 0.0028\n",
            "Iter: 4999  \tTraining Loss: -492.9196    \n",
            "    Negative Log Likelihood: 30.4518\tSigma2 Prior: -523.3741\tRegularization: 0.0028\n",
            "Done training with 5000 iterations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_iOpN8KJRHC"
      },
      "source": [
        "## 모델평가\n",
        "- 평가 데이터를 사용해 각 sequence에 해당하는 화자id를 얻음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kI1oSISRJ3E7",
        "outputId": "ded457ac-6586-4b60-8a7e-c6dd731ad666"
      },
      "source": [
        "predicted_cluster_ids = []\n",
        "test_record = []\n",
        "\n",
        "for (test_sequence, test_cluster_id) in zip(test_sequences, test_cluster_ids):\n",
        "  predicted_cluster_id = model.predict(test_sequence, inference_args)\n",
        "  predicted_cluster_ids.append(predicted_cluster_id) #결과통계내려고\n",
        "  accuracy = uisrnn.compute_sequence_match_accuracy(test_cluster_id, predicted_cluster_id)#정답레이블과 예측한id를 맞춤\n",
        "  test_record.append((accuracy, len(test_cluster_id)))\n",
        "  print('Ground truth labels:')\n",
        "  print(test_cluster_id)\n",
        "  print('Predicted labels')\n",
        "  print(predicted_cluster_id)\n",
        "  print('-' * 100 )\n",
        "\n",
        "output_result = uisrnn.output_result(model_args, training_args, test_record)\n",
        "print(output_result)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ground truth labels:\n",
            "['15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_2', '75_2', '75_2', '75_2', '75_2', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_3', '75_3', '75_3', '75_3', '75_3', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_2', '75_2', '75_2']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_2', '140_2', '140_2', '140_2', '140_2', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_1', '175_1', '175_1', '175_1']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_5', '315_5', '315_5', '315_5']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1']\n",
            "Predicted labels\n",
            "[0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 2]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_2', '439_2', '439_2', '439_2', '439_2', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_2', '439_2', '439_2', '439_2', '439_2', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ground truth labels:\n",
            "['494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_2', '494_2', '494_2', '494_2', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0']\n",
            "Predicted labels\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3502a319fc2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0moutput_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muisrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpritn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pritn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5qBdCe4LA59",
        "outputId": "ab873c83-fe75-42a0-b505-e0e974596e4c"
      },
      "source": [
        "output_result = uisrnn.output_result(model_args, training_args, test_record)\n",
        "print(output_result)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config:\n",
            "  sigma_alpha: 1.0\n",
            "  sigma_beta: 1.0\n",
            "  crp_alpha: 1.0\n",
            "  learning rate: 0.001\n",
            "  regularization: 1e-05\n",
            "  batch size: 10\n",
            "\n",
            "Performance:\n",
            "  averaged accuracy: 0.998022\n",
            "  accuracy numbers for all testing sequences:\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    0.989362\n",
            "    1.000000\n",
            "    1.000000\n",
            "    0.989583\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    0.989362\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    0.991597\n",
            "    0.990654\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}